{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. gas_price\n",
      "2. rig_count\n",
      "3. tam\n",
      "4. capex\n",
      "5. oil_price\n",
      "6. education\n",
      "7. political_stability\n",
      "8. infrastructure_index\n",
      "9. health_index\n",
      "10. gni\n",
      "11. gdp\n",
      "12. population\n",
      "13. country_mapping\n",
      "\n",
      "Available Datasets for Prediction, Here's a catch - If you select all relevant data then predictions is on basis of global indicators else the neural network identify correlation itself adding historical considerations to predict\n",
      "\n",
      "1. Gas Price Prediction: Uses Oil Price, Rig Count, Infrastructure Index, Capex.\n",
      "2. Rig Count Prediction: Uses Rig Count, Infrastructure Index, Capex, Political Stability.\n",
      "3. Oil Price Prediction: Uses Rig Count, GDP Growth, Infrastructure Index, Capex, Political Stability.\n",
      "4. Tam Prediction: Uses Capex, GDP, Population, Health Index, Education, Infrastructure Index.\n",
      "5. Capex Prediction: Uses Oil Price, Gas Price, Rig Count, GDP Growth, Political Stability, Infrastructure Index.\n",
      "\n",
      "Available countries: AFGHANISTAN, ALBANIA, ALGERIA, ANDORRA, ANGOLA, ANTIGUA & BARBUDA, ARGENTINA, ARMENIA, AUSTRALIA, AUSTRIA, AZERBAIJAN, BAHRAIN, BANGLADESH, BELARUS, BELGIUM, BELIZE, BENIN, BHUTAN, BOLIVIA, BOSNIA & HERZEGOVINA, BOTSWANA, BRAZIL, BRUNEI DARUSSALAM, BULGARIA, BURKINA FASO, BURUNDI, CAMBODIA, CAMEROON, CANADA, CHAD, CHILE, CHINA, COLOMBIA, CONGO, DEM. REP., COSTA RICA, COTE D'IVOIRE, CROATIA, CUBA, CYPRUS, CZECH REPUBLIC, DENMARK, DJIBOUTI, DOMINICAN REPUBLIC, ECUADOR, EGYPT, EL SALVADOR, EQUATORIAL GUINEA, ESTONIA, ETHIOPIA, FINLAND, FRANCE, GAMBIA, GEORGIA, GERMANY, GHANA, GREECE, GUATEMALA, GUINEA, GUINEA-BISSAU, GUYANA, HAITI, HONDURAS, HUNGARY, ICELAND, INDIA, INDONESIA, IRAN, IRAQ, IRELAND, ISRAEL, ITALY, JAMAICA, JAPAN, JORDAN, KAZAKHSTAN, KENYA, KIRIBATI, KUWAIT, KYRGYZ REPUBLIC, LAOS, LATVIA, LEBANON, LESOTHO, LIBERIA, LIBYA, LITHUANIA, LUXEMBOURG, MADAGASCAR, MALAWI, MALAYSIA, MALDIVES, MALI, MALTA, MAURITANIA, MAURITIUS, MEXICO, MICRONESIA, MOLDOVA, MONACO, MONGOLIA, MONTENEGRO, MOROCCO, MOZAMBIQUE, MYANMAR, NAMIBIA, NEPAL, NETHERLANDS, NEW CALEDONIA, NEW ZEALAND, NICARAGUA, NIGER, NIGERIA, NORTH MACEDONIA, NORWAY, OMAN, PAKISTAN, PANAMA, PAPUA NEW GUINEA, PARAGUAY, PERU, PHILIPPINES, POLAND, PORTUGAL, PUERTO RICO, QATAR, ROMANIA, RUSSIA, RWANDA, SAN MARINO, SAUDI ARABIA, SENEGAL, SERBIA, SIERRA LEONE, SINGAPORE, SLOVAKIA, SLOVENIA, SOMALIA, SOUTH AFRICA, SOUTH KOREA, SOUTH SUDAN, SPAIN, SRI LANKA, SUDAN, SWEDEN, SWITZERLAND, SYRIAN ARAB REPUBLIC, TAJIKISTAN, TANZANIA, THAILAND, THE BAHAMAS, TIMOR-LESTE, TOGO, TONGA, TRINIDAD AND TOBAGO, TUNISIA, TURKEY, UGANDA, UKRAINE, UNITED ARAB EMIRATES, UNITED KINGDOM, UNITED STATES, URUGUAY, VANUATU, VENEZUELA, VIET NAM, WEST BANK AND GAZA, YEMEN, ZAMBIA, ZIMBABWE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 - 6s - 6s/step - loss: 3.2902 - mae: 1.5119 - val_loss: 0.2890 - val_mae: 0.4418 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "1/1 - 0s - 287ms/step - loss: 1.4806 - mae: 1.0377 - val_loss: 0.2813 - val_mae: 0.4324 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "1/1 - 0s - 329ms/step - loss: 1.8041 - mae: 1.0239 - val_loss: 0.2741 - val_mae: 0.4239 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "1/1 - 0s - 264ms/step - loss: 3.3200 - mae: 1.6107 - val_loss: 0.2667 - val_mae: 0.4143 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "1/1 - 0s - 274ms/step - loss: 1.6403 - mae: 1.0170 - val_loss: 0.2592 - val_mae: 0.4044 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "1/1 - 0s - 292ms/step - loss: 1.5099 - mae: 0.9267 - val_loss: 0.2535 - val_mae: 0.3964 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "1/1 - 0s - 286ms/step - loss: 4.4967 - mae: 1.3430 - val_loss: 0.2476 - val_mae: 0.3876 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "1/1 - 0s - 279ms/step - loss: 1.3824 - mae: 0.9547 - val_loss: 0.2422 - val_mae: 0.3788 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "1/1 - 0s - 337ms/step - loss: 1.2833 - mae: 0.8086 - val_loss: 0.2369 - val_mae: 0.3699 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "1/1 - 0s - 271ms/step - loss: 2.3333 - mae: 1.1854 - val_loss: 0.2333 - val_mae: 0.3631 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "1/1 - 0s - 280ms/step - loss: 1.8753 - mae: 1.1002 - val_loss: 0.2288 - val_mae: 0.3552 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "1/1 - 0s - 286ms/step - loss: 1.3420 - mae: 0.9026 - val_loss: 0.2252 - val_mae: 0.3484 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "1/1 - 0s - 311ms/step - loss: 2.2455 - mae: 1.1135 - val_loss: 0.2216 - val_mae: 0.3409 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "1/1 - 0s - 271ms/step - loss: 1.8264 - mae: 1.0776 - val_loss: 0.2177 - val_mae: 0.3330 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "1/1 - 0s - 349ms/step - loss: 1.1554 - mae: 0.8247 - val_loss: 0.2150 - val_mae: 0.3274 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "1/1 - 0s - 320ms/step - loss: 1.5137 - mae: 0.9898 - val_loss: 0.2120 - val_mae: 0.3213 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "1/1 - 0s - 259ms/step - loss: 1.7186 - mae: 0.9446 - val_loss: 0.2090 - val_mae: 0.3156 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "1/1 - 0s - 255ms/step - loss: 1.6713 - mae: 0.9964 - val_loss: 0.2066 - val_mae: 0.3111 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "1/1 - 0s - 252ms/step - loss: 3.0234 - mae: 1.2922 - val_loss: 0.2053 - val_mae: 0.3084 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "1/1 - 0s - 288ms/step - loss: 1.1283 - mae: 0.8270 - val_loss: 0.2038 - val_mae: 0.3057 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "1/1 - 0s - 282ms/step - loss: 1.1348 - mae: 0.8624 - val_loss: 0.2023 - val_mae: 0.3037 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "1/1 - 0s - 255ms/step - loss: 2.2085 - mae: 1.0931 - val_loss: 0.2011 - val_mae: 0.3022 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "1/1 - 0s - 280ms/step - loss: 1.7300 - mae: 1.0377 - val_loss: 0.2002 - val_mae: 0.3017 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "1/1 - 0s - 256ms/step - loss: 1.2882 - mae: 0.9178 - val_loss: 0.1986 - val_mae: 0.2996 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "1/1 - 0s - 258ms/step - loss: 1.2368 - mae: 0.9338 - val_loss: 0.1970 - val_mae: 0.2978 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "1/1 - 0s - 251ms/step - loss: 1.9040 - mae: 1.1494 - val_loss: 0.1957 - val_mae: 0.2969 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "1/1 - 0s - 257ms/step - loss: 1.1641 - mae: 0.7638 - val_loss: 0.1943 - val_mae: 0.2951 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "1/1 - 0s - 269ms/step - loss: 1.6701 - mae: 1.0824 - val_loss: 0.1928 - val_mae: 0.2931 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "1/1 - 0s - 279ms/step - loss: 1.1541 - mae: 0.7718 - val_loss: 0.1913 - val_mae: 0.2910 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "1/1 - 0s - 283ms/step - loss: 2.1793 - mae: 1.0965 - val_loss: 0.1896 - val_mae: 0.2886 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "1/1 - 0s - 321ms/step - loss: 1.3975 - mae: 0.9321 - val_loss: 0.1882 - val_mae: 0.2860 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "1/1 - 0s - 253ms/step - loss: 1.1950 - mae: 0.8577 - val_loss: 0.1871 - val_mae: 0.2841 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "1/1 - 1s - 690ms/step - loss: 1.1148 - mae: 0.8738 - val_loss: 0.1853 - val_mae: 0.2807 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "1/1 - 0s - 319ms/step - loss: 2.2627 - mae: 1.0452 - val_loss: 0.1833 - val_mae: 0.2765 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "1/1 - 0s - 264ms/step - loss: 1.3958 - mae: 0.9151 - val_loss: 0.1823 - val_mae: 0.2738 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "1/1 - 0s - 312ms/step - loss: 1.5759 - mae: 1.0375 - val_loss: 0.1810 - val_mae: 0.2702 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "1/1 - 0s - 329ms/step - loss: 1.6365 - mae: 1.0673 - val_loss: 0.1803 - val_mae: 0.2673 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "1/1 - 0s - 348ms/step - loss: 1.2921 - mae: 0.9279 - val_loss: 0.1792 - val_mae: 0.2652 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "1/1 - 0s - 258ms/step - loss: 1.8573 - mae: 1.0524 - val_loss: 0.1786 - val_mae: 0.2659 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "1/1 - 0s - 253ms/step - loss: 1.2862 - mae: 0.8054 - val_loss: 0.1785 - val_mae: 0.2669 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "1/1 - 0s - 268ms/step - loss: 1.5222 - mae: 0.8934 - val_loss: 0.1787 - val_mae: 0.2678 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "1/1 - 0s - 245ms/step - loss: 1.1791 - mae: 0.9792 - val_loss: 0.1795 - val_mae: 0.2689 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "1/1 - 0s - 344ms/step - loss: 1.2830 - mae: 0.8228 - val_loss: 0.1803 - val_mae: 0.2703 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1/1 - 0s - 315ms/step - loss: 0.6673 - mae: 0.6688 - val_loss: 0.1801 - val_mae: 0.2711 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "1/1 - 0s - 337ms/step - loss: 0.9411 - mae: 0.7163 - val_loss: 0.1793 - val_mae: 0.2712 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "1/1 - 0s - 245ms/step - loss: 0.6968 - mae: 0.6306 - val_loss: 0.1786 - val_mae: 0.2714 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "1/1 - 0s - 244ms/step - loss: 1.1047 - mae: 0.8351 - val_loss: 0.1784 - val_mae: 0.2717 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "1/1 - 0s - 266ms/step - loss: 0.6980 - mae: 0.6607 - val_loss: 0.1785 - val_mae: 0.2721 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "1/1 - 0s - 255ms/step - loss: 0.3748 - mae: 0.4977 - val_loss: 0.1788 - val_mae: 0.2726 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "1/1 - 0s - 325ms/step - loss: 1.5164 - mae: 0.9644 - val_loss: 0.1786 - val_mae: 0.2729 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 47.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 - 6s - 6s/step - loss: 2.0282 - mae: 1.0119 - val_loss: 0.2165 - val_mae: 0.3265 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "1/1 - 0s - 312ms/step - loss: 1.9248 - mae: 1.0299 - val_loss: 0.2182 - val_mae: 0.3304 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "1/1 - 0s - 328ms/step - loss: 1.8508 - mae: 1.1677 - val_loss: 0.2170 - val_mae: 0.3307 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "1/1 - 0s - 275ms/step - loss: 1.5842 - mae: 0.9324 - val_loss: 0.2144 - val_mae: 0.3291 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "1/1 - 0s - 292ms/step - loss: 1.1119 - mae: 0.8241 - val_loss: 0.2112 - val_mae: 0.3268 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "1/1 - 0s - 325ms/step - loss: 2.6851 - mae: 1.3825 - val_loss: 0.2077 - val_mae: 0.3236 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "1/1 - 0s - 289ms/step - loss: 0.9585 - mae: 0.8730 - val_loss: 0.2048 - val_mae: 0.3211 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "1/1 - 0s - 282ms/step - loss: 1.1581 - mae: 0.9406 - val_loss: 0.2012 - val_mae: 0.3177 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "1/1 - 0s - 274ms/step - loss: 1.5658 - mae: 1.1159 - val_loss: 0.1975 - val_mae: 0.3136 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "1/1 - 1s - 666ms/step - loss: 1.7931 - mae: 1.1061 - val_loss: 0.1929 - val_mae: 0.3081 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "1/1 - 0s - 282ms/step - loss: 2.1857 - mae: 1.2977 - val_loss: 0.1878 - val_mae: 0.3020 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "1/1 - 0s - 337ms/step - loss: 1.7616 - mae: 1.0230 - val_loss: 0.1841 - val_mae: 0.2973 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "1/1 - 0s - 280ms/step - loss: 1.4252 - mae: 0.9706 - val_loss: 0.1805 - val_mae: 0.2928 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "1/1 - 0s - 324ms/step - loss: 1.1952 - mae: 0.9526 - val_loss: 0.1773 - val_mae: 0.2888 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "1/1 - 0s - 317ms/step - loss: 1.7964 - mae: 1.1331 - val_loss: 0.1747 - val_mae: 0.2855 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "1/1 - 0s - 283ms/step - loss: 1.9510 - mae: 1.1333 - val_loss: 0.1721 - val_mae: 0.2823 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "1/1 - 0s - 279ms/step - loss: 1.6379 - mae: 1.0746 - val_loss: 0.1687 - val_mae: 0.2778 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "1/1 - 0s - 248ms/step - loss: 1.1369 - mae: 0.7798 - val_loss: 0.1659 - val_mae: 0.2739 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "1/1 - 0s - 338ms/step - loss: 1.5470 - mae: 0.9621 - val_loss: 0.1629 - val_mae: 0.2695 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "1/1 - 0s - 268ms/step - loss: 0.9266 - mae: 0.7809 - val_loss: 0.1603 - val_mae: 0.2657 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "1/1 - 0s - 336ms/step - loss: 0.8432 - mae: 0.7372 - val_loss: 0.1575 - val_mae: 0.2616 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "1/1 - 0s - 350ms/step - loss: 1.2932 - mae: 0.8707 - val_loss: 0.1559 - val_mae: 0.2591 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "1/1 - 0s - 299ms/step - loss: 1.1632 - mae: 0.9504 - val_loss: 0.1536 - val_mae: 0.2558 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "1/1 - 0s - 258ms/step - loss: 1.3414 - mae: 0.9255 - val_loss: 0.1512 - val_mae: 0.2523 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "1/1 - 0s - 267ms/step - loss: 1.2148 - mae: 0.9013 - val_loss: 0.1490 - val_mae: 0.2488 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "1/1 - 0s - 266ms/step - loss: 0.9211 - mae: 0.7566 - val_loss: 0.1470 - val_mae: 0.2455 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "1/1 - 0s - 255ms/step - loss: 0.5551 - mae: 0.5903 - val_loss: 0.1452 - val_mae: 0.2425 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "1/1 - 0s - 323ms/step - loss: 0.6483 - mae: 0.6378 - val_loss: 0.1430 - val_mae: 0.2386 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "1/1 - 0s - 335ms/step - loss: 0.6138 - mae: 0.7108 - val_loss: 0.1410 - val_mae: 0.2348 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "1/1 - 0s - 316ms/step - loss: 0.8562 - mae: 0.6773 - val_loss: 0.1394 - val_mae: 0.2315 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "1/1 - 0s - 284ms/step - loss: 1.4567 - mae: 0.9342 - val_loss: 0.1380 - val_mae: 0.2306 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "1/1 - 0s - 240ms/step - loss: 1.7496 - mae: 1.0755 - val_loss: 0.1366 - val_mae: 0.2325 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "1/1 - 0s - 253ms/step - loss: 0.4393 - mae: 0.5412 - val_loss: 0.1354 - val_mae: 0.2342 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "1/1 - 0s - 264ms/step - loss: 1.2346 - mae: 0.9230 - val_loss: 0.1344 - val_mae: 0.2359 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "1/1 - 0s - 260ms/step - loss: 0.7248 - mae: 0.7080 - val_loss: 0.1335 - val_mae: 0.2380 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "1/1 - 0s - 259ms/step - loss: 0.7077 - mae: 0.6799 - val_loss: 0.1327 - val_mae: 0.2402 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "1/1 - 0s - 285ms/step - loss: 0.8856 - mae: 0.7542 - val_loss: 0.1322 - val_mae: 0.2423 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "1/1 - 0s - 272ms/step - loss: 0.7559 - mae: 0.6992 - val_loss: 0.1318 - val_mae: 0.2442 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "1/1 - 0s - 269ms/step - loss: 0.4826 - mae: 0.4973 - val_loss: 0.1314 - val_mae: 0.2459 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "1/1 - 0s - 325ms/step - loss: 0.8589 - mae: 0.7706 - val_loss: 0.1312 - val_mae: 0.2478 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "1/1 - 0s - 330ms/step - loss: 0.7507 - mae: 0.6534 - val_loss: 0.1309 - val_mae: 0.2496 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "1/1 - 0s - 252ms/step - loss: 0.8533 - mae: 0.7288 - val_loss: 0.1303 - val_mae: 0.2512 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "1/1 - 0s - 257ms/step - loss: 0.7521 - mae: 0.7577 - val_loss: 0.1297 - val_mae: 0.2529 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "1/1 - 0s - 456ms/step - loss: 1.0041 - mae: 0.8009 - val_loss: 0.1294 - val_mae: 0.2543 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "1/1 - 0s - 247ms/step - loss: 0.6612 - mae: 0.5887 - val_loss: 0.1292 - val_mae: 0.2557 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "1/1 - 0s - 254ms/step - loss: 1.2538 - mae: 0.9003 - val_loss: 0.1290 - val_mae: 0.2574 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "1/1 - 0s - 313ms/step - loss: 0.6615 - mae: 0.5614 - val_loss: 0.1288 - val_mae: 0.2591 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "1/1 - 0s - 264ms/step - loss: 0.8779 - mae: 0.7758 - val_loss: 0.1284 - val_mae: 0.2606 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "1/1 - 0s - 264ms/step - loss: 0.4820 - mae: 0.6084 - val_loss: 0.1280 - val_mae: 0.2620 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "1/1 - 0s - 257ms/step - loss: 1.1559 - mae: 0.7558 - val_loss: 0.1276 - val_mae: 0.2631 - learning_rate: 0.0010\n",
      "Restoring model weights from the end of the best epoch: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_10148\\3858047369.py:85: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  country_df_sorted.interpolate(method=\"linear\", inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 - 7s - 7s/step - loss: 2.6172 - mae: 1.1240 - val_loss: 0.1717 - val_mae: 0.3211 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "1/1 - 1s - 1s/step - loss: 1.1512 - mae: 0.8272 - val_loss: 0.1668 - val_mae: 0.3164 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "1/1 - 0s - 336ms/step - loss: 0.7536 - mae: 0.5829 - val_loss: 0.1606 - val_mae: 0.3106 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "1/1 - 0s - 390ms/step - loss: 1.0324 - mae: 0.8495 - val_loss: 0.1550 - val_mae: 0.3052 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "1/1 - 0s - 301ms/step - loss: 1.0534 - mae: 0.7383 - val_loss: 0.1490 - val_mae: 0.3005 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "1/1 - 0s - 270ms/step - loss: 0.5675 - mae: 0.6277 - val_loss: 0.1442 - val_mae: 0.2982 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "1/1 - 0s - 293ms/step - loss: 1.3551 - mae: 0.8269 - val_loss: 0.1397 - val_mae: 0.2955 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "1/1 - 0s - 289ms/step - loss: 1.3382 - mae: 0.8451 - val_loss: 0.1347 - val_mae: 0.2922 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "1/1 - 0s - 291ms/step - loss: 0.9382 - mae: 0.7015 - val_loss: 0.1298 - val_mae: 0.2887 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "1/1 - 0s - 277ms/step - loss: 0.7089 - mae: 0.5662 - val_loss: 0.1253 - val_mae: 0.2855 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "1/1 - 0s - 352ms/step - loss: 1.3209 - mae: 0.7857 - val_loss: 0.1212 - val_mae: 0.2822 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "1/1 - 0s - 333ms/step - loss: 0.7694 - mae: 0.6963 - val_loss: 0.1168 - val_mae: 0.2790 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "1/1 - 0s - 298ms/step - loss: 1.2930 - mae: 0.8843 - val_loss: 0.1127 - val_mae: 0.2761 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "1/1 - 0s - 325ms/step - loss: 0.8253 - mae: 0.7406 - val_loss: 0.1096 - val_mae: 0.2741 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "1/1 - 0s - 316ms/step - loss: 0.9767 - mae: 0.8217 - val_loss: 0.1063 - val_mae: 0.2722 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "1/1 - 0s - 303ms/step - loss: 0.6145 - mae: 0.6146 - val_loss: 0.1035 - val_mae: 0.2705 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "1/1 - 0s - 318ms/step - loss: 0.9667 - mae: 0.7312 - val_loss: 0.0997 - val_mae: 0.2673 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "1/1 - 0s - 248ms/step - loss: 1.0124 - mae: 0.7045 - val_loss: 0.0954 - val_mae: 0.2638 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "1/1 - 0s - 434ms/step - loss: 0.5833 - mae: 0.5354 - val_loss: 0.0914 - val_mae: 0.2605 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "1/1 - 1s - 520ms/step - loss: 0.3861 - mae: 0.5004 - val_loss: 0.0882 - val_mae: 0.2577 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "1/1 - 0s - 279ms/step - loss: 0.5280 - mae: 0.5173 - val_loss: 0.0857 - val_mae: 0.2555 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "1/1 - 0s - 319ms/step - loss: 0.8099 - mae: 0.6900 - val_loss: 0.0835 - val_mae: 0.2538 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "1/1 - 0s - 283ms/step - loss: 0.9495 - mae: 0.6832 - val_loss: 0.0807 - val_mae: 0.2512 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "1/1 - 0s - 276ms/step - loss: 0.5166 - mae: 0.6079 - val_loss: 0.0783 - val_mae: 0.2489 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "1/1 - 0s - 247ms/step - loss: 1.6286 - mae: 0.9074 - val_loss: 0.0758 - val_mae: 0.2461 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "1/1 - 0s - 255ms/step - loss: 1.3241 - mae: 0.7515 - val_loss: 0.0729 - val_mae: 0.2426 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "1/1 - 0s - 287ms/step - loss: 0.5984 - mae: 0.6653 - val_loss: 0.0705 - val_mae: 0.2396 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "1/1 - 0s - 275ms/step - loss: 0.3954 - mae: 0.5170 - val_loss: 0.0685 - val_mae: 0.2371 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "1/1 - 0s - 245ms/step - loss: 0.8048 - mae: 0.5725 - val_loss: 0.0668 - val_mae: 0.2348 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "1/1 - 0s - 356ms/step - loss: 1.4088 - mae: 0.8355 - val_loss: 0.0654 - val_mae: 0.2331 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "1/1 - 0s - 312ms/step - loss: 1.5303 - mae: 0.9052 - val_loss: 0.0640 - val_mae: 0.2314 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "1/1 - 0s - 278ms/step - loss: 0.7228 - mae: 0.6956 - val_loss: 0.0624 - val_mae: 0.2294 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "1/1 - 0s - 251ms/step - loss: 1.0255 - mae: 0.7760 - val_loss: 0.0609 - val_mae: 0.2273 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "1/1 - 0s - 247ms/step - loss: 0.8350 - mae: 0.6246 - val_loss: 0.0594 - val_mae: 0.2253 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "1/1 - 0s - 371ms/step - loss: 0.6759 - mae: 0.6315 - val_loss: 0.0586 - val_mae: 0.2241 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "1/1 - 0s - 259ms/step - loss: 0.5415 - mae: 0.5445 - val_loss: 0.0578 - val_mae: 0.2235 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "1/1 - 0s - 266ms/step - loss: 0.7766 - mae: 0.6439 - val_loss: 0.0571 - val_mae: 0.2227 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "1/1 - 0s - 337ms/step - loss: 0.3411 - mae: 0.3956 - val_loss: 0.0566 - val_mae: 0.2220 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "1/1 - 0s - 255ms/step - loss: 0.9910 - mae: 0.7368 - val_loss: 0.0559 - val_mae: 0.2210 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "1/1 - 0s - 243ms/step - loss: 0.2692 - mae: 0.3942 - val_loss: 0.0550 - val_mae: 0.2197 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "1/1 - 0s - 245ms/step - loss: 0.9388 - mae: 0.6456 - val_loss: 0.0545 - val_mae: 0.2192 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "1/1 - 0s - 251ms/step - loss: 0.5687 - mae: 0.5149 - val_loss: 0.0539 - val_mae: 0.2188 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "1/1 - 0s - 235ms/step - loss: 0.8555 - mae: 0.6594 - val_loss: 0.0538 - val_mae: 0.2189 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "1/1 - 0s - 369ms/step - loss: 0.5852 - mae: 0.6270 - val_loss: 0.0535 - val_mae: 0.2190 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "1/1 - 0s - 263ms/step - loss: 1.3599 - mae: 0.7111 - val_loss: 0.0532 - val_mae: 0.2190 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "1/1 - 0s - 239ms/step - loss: 0.9650 - mae: 0.6165 - val_loss: 0.0531 - val_mae: 0.2192 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "1/1 - 0s - 250ms/step - loss: 0.5601 - mae: 0.5343 - val_loss: 0.0528 - val_mae: 0.2192 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "1/1 - 0s - 329ms/step - loss: 0.8690 - mae: 0.7292 - val_loss: 0.0528 - val_mae: 0.2196 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "1/1 - 0s - 349ms/step - loss: 0.7691 - mae: 0.6087 - val_loss: 0.0527 - val_mae: 0.2200 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "1/1 - 0s - 235ms/step - loss: 0.3404 - mae: 0.4800 - val_loss: 0.0531 - val_mae: 0.2211 - learning_rate: 0.0010\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Predicted tam for ALGERIA in 2027: 7.785793276444818\n",
      "Predicted tam for ALGERIA in 2028: 7.6738495694878415\n",
      "Predicted tam for ALGERIA in 2029: 7.999789529303996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_10148\\3858047369.py:85: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  country_df_sorted.interpolate(method=\"linear\", inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 - 6s - 6s/step - loss: 1.8623 - mae: 1.0336 - val_loss: 0.1213 - val_mae: 0.2662 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "1/1 - 1s - 751ms/step - loss: 1.7274 - mae: 1.0817 - val_loss: 0.1204 - val_mae: 0.2670 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "1/1 - 0s - 332ms/step - loss: 2.8287 - mae: 1.1587 - val_loss: 0.1224 - val_mae: 0.2721 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "1/1 - 0s - 312ms/step - loss: 1.7972 - mae: 0.9692 - val_loss: 0.1235 - val_mae: 0.2772 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "1/1 - 0s - 293ms/step - loss: 2.7365 - mae: 1.3363 - val_loss: 0.1246 - val_mae: 0.2827 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "1/1 - 0s - 319ms/step - loss: 1.2824 - mae: 0.8956 - val_loss: 0.1254 - val_mae: 0.2873 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1/1 - 0s - 280ms/step - loss: 1.7071 - mae: 0.9773 - val_loss: 0.1260 - val_mae: 0.2916 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "1/1 - 0s - 303ms/step - loss: 2.2053 - mae: 1.2003 - val_loss: 0.1265 - val_mae: 0.2938 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "1/1 - 0s - 285ms/step - loss: 1.3656 - mae: 0.9119 - val_loss: 0.1269 - val_mae: 0.2958 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "1/1 - 0s - 285ms/step - loss: 1.7905 - mae: 0.9338 - val_loss: 0.1259 - val_mae: 0.2970 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "1/1 - 0s - 262ms/step - loss: 0.7428 - mae: 0.6881 - val_loss: 0.1262 - val_mae: 0.2987 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1/1 - 0s - 299ms/step - loss: 0.8793 - mae: 0.8098 - val_loss: 0.1261 - val_mae: 0.3002 - learning_rate: 5.0000e-04\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Predicted tam for INDIA in 2027: 0.22079130818748044\n",
      "Predicted tam for INDIA in 2028: 0.21017363183516022\n",
      "Predicted tam for INDIA in 2029: 0.2102506169273897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_10148\\3858047369.py:85: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  country_df_sorted.interpolate(method=\"linear\", inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 - 6s - 6s/step - loss: 2.3017 - mae: 1.3412 - val_loss: 0.1495 - val_mae: 0.3005 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "1/1 - 0s - 295ms/step - loss: 2.1443 - mae: 1.2814 - val_loss: 0.1546 - val_mae: 0.3057 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "1/1 - 0s - 333ms/step - loss: 1.2621 - mae: 0.9809 - val_loss: 0.1548 - val_mae: 0.3060 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "1/1 - 0s - 330ms/step - loss: 3.3923 - mae: 1.4109 - val_loss: 0.1514 - val_mae: 0.3064 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "1/1 - 0s - 296ms/step - loss: 1.3264 - mae: 0.9423 - val_loss: 0.1473 - val_mae: 0.3055 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "1/1 - 0s - 300ms/step - loss: 2.0547 - mae: 1.3362 - val_loss: 0.1438 - val_mae: 0.3045 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "1/1 - 0s - 309ms/step - loss: 3.4196 - mae: 1.6466 - val_loss: 0.1407 - val_mae: 0.3037 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "1/1 - 0s - 316ms/step - loss: 3.2816 - mae: 1.5989 - val_loss: 0.1372 - val_mae: 0.3027 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "1/1 - 0s - 351ms/step - loss: 2.4473 - mae: 1.3068 - val_loss: 0.1347 - val_mae: 0.3028 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "1/1 - 0s - 316ms/step - loss: 1.8205 - mae: 1.1240 - val_loss: 0.1322 - val_mae: 0.3028 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "1/1 - 1s - 715ms/step - loss: 1.2017 - mae: 1.0244 - val_loss: 0.1294 - val_mae: 0.3024 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "1/1 - 0s - 295ms/step - loss: 2.0879 - mae: 1.1581 - val_loss: 0.1265 - val_mae: 0.3015 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "1/1 - 0s - 305ms/step - loss: 2.1892 - mae: 1.1231 - val_loss: 0.1229 - val_mae: 0.2997 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "1/1 - 0s - 311ms/step - loss: 1.7289 - mae: 1.1098 - val_loss: 0.1185 - val_mae: 0.2973 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "1/1 - 0s - 333ms/step - loss: 2.5134 - mae: 1.2580 - val_loss: 0.1137 - val_mae: 0.2942 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "1/1 - 0s - 299ms/step - loss: 1.5538 - mae: 1.1220 - val_loss: 0.1097 - val_mae: 0.2913 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "1/1 - 0s - 294ms/step - loss: 1.4257 - mae: 1.0348 - val_loss: 0.1053 - val_mae: 0.2880 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "1/1 - 0s - 303ms/step - loss: 0.9205 - mae: 0.8026 - val_loss: 0.1013 - val_mae: 0.2849 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "1/1 - 0s - 243ms/step - loss: 1.9372 - mae: 1.1970 - val_loss: 0.0976 - val_mae: 0.2820 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "1/1 - 0s - 256ms/step - loss: 1.3226 - mae: 0.9805 - val_loss: 0.0940 - val_mae: 0.2793 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "1/1 - 0s - 263ms/step - loss: 1.6349 - mae: 0.9992 - val_loss: 0.0905 - val_mae: 0.2763 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "1/1 - 0s - 274ms/step - loss: 1.1880 - mae: 0.9919 - val_loss: 0.0874 - val_mae: 0.2737 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "1/1 - 0s - 306ms/step - loss: 0.8089 - mae: 0.7293 - val_loss: 0.0849 - val_mae: 0.2718 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "1/1 - 0s - 238ms/step - loss: 1.0815 - mae: 0.8342 - val_loss: 0.0830 - val_mae: 0.2706 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "1/1 - 0s - 256ms/step - loss: 1.0240 - mae: 0.7105 - val_loss: 0.0810 - val_mae: 0.2691 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "1/1 - 0s - 255ms/step - loss: 2.1316 - mae: 1.1731 - val_loss: 0.0791 - val_mae: 0.2675 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "1/1 - 0s - 278ms/step - loss: 1.2280 - mae: 0.9059 - val_loss: 0.0775 - val_mae: 0.2659 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "1/1 - 0s - 275ms/step - loss: 0.8696 - mae: 0.8002 - val_loss: 0.0760 - val_mae: 0.2644 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "1/1 - 0s - 331ms/step - loss: 2.4992 - mae: 1.1698 - val_loss: 0.0744 - val_mae: 0.2625 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "1/1 - 0s - 256ms/step - loss: 0.7766 - mae: 0.7349 - val_loss: 0.0728 - val_mae: 0.2607 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "1/1 - 0s - 267ms/step - loss: 1.2879 - mae: 0.9754 - val_loss: 0.0715 - val_mae: 0.2591 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "1/1 - 0s - 257ms/step - loss: 1.5409 - mae: 0.9476 - val_loss: 0.0707 - val_mae: 0.2582 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "1/1 - 0s - 319ms/step - loss: 0.9609 - mae: 0.8748 - val_loss: 0.0699 - val_mae: 0.2572 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "1/1 - 0s - 269ms/step - loss: 2.2313 - mae: 1.3151 - val_loss: 0.0690 - val_mae: 0.2556 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "1/1 - 0s - 258ms/step - loss: 1.4275 - mae: 0.9883 - val_loss: 0.0678 - val_mae: 0.2533 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "1/1 - 0s - 312ms/step - loss: 0.9981 - mae: 0.7651 - val_loss: 0.0665 - val_mae: 0.2507 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "1/1 - 0s - 267ms/step - loss: 1.3240 - mae: 0.9441 - val_loss: 0.0654 - val_mae: 0.2480 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "1/1 - 0s - 249ms/step - loss: 0.6918 - mae: 0.7065 - val_loss: 0.0648 - val_mae: 0.2459 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "1/1 - 0s - 248ms/step - loss: 1.2183 - mae: 0.8373 - val_loss: 0.0645 - val_mae: 0.2444 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "1/1 - 0s - 247ms/step - loss: 1.2971 - mae: 0.8807 - val_loss: 0.0641 - val_mae: 0.2424 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "1/1 - 0s - 254ms/step - loss: 0.5915 - mae: 0.6145 - val_loss: 0.0640 - val_mae: 0.2404 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "1/1 - 0s - 234ms/step - loss: 0.8612 - mae: 0.8053 - val_loss: 0.0640 - val_mae: 0.2388 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "1/1 - 0s - 346ms/step - loss: 0.8410 - mae: 0.7540 - val_loss: 0.0643 - val_mae: 0.2377 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "1/1 - 0s - 239ms/step - loss: 1.1713 - mae: 0.8714 - val_loss: 0.0649 - val_mae: 0.2361 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "1/1 - 1s - 538ms/step - loss: 0.8932 - mae: 0.8269 - val_loss: 0.0652 - val_mae: 0.2345 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1/1 - 0s - 248ms/step - loss: 0.8778 - mae: 0.8582 - val_loss: 0.0658 - val_mae: 0.2327 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "1/1 - 0s - 330ms/step - loss: 0.6867 - mae: 0.6814 - val_loss: 0.0662 - val_mae: 0.2320 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "1/1 - 0s - 340ms/step - loss: 1.2850 - mae: 0.8678 - val_loss: 0.0664 - val_mae: 0.2311 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "1/1 - 0s - 248ms/step - loss: 0.5948 - mae: 0.6179 - val_loss: 0.0667 - val_mae: 0.2305 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "1/1 - 0s - 330ms/step - loss: 1.2118 - mae: 0.8375 - val_loss: 0.0671 - val_mae: 0.2302 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "Predicted capex for ALGERIA in 2027: 5.960923225629843\n",
      "Predicted capex for ALGERIA in 2028: 6.905138829786204\n",
      "Predicted capex for ALGERIA in 2029: 6.125709601736193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_10148\\3858047369.py:85: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  country_df_sorted.interpolate(method=\"linear\", inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 - 6s - 6s/step - loss: 2.1514 - mae: 1.2176 - val_loss: 0.0774 - val_mae: 0.2138 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "1/1 - 0s - 311ms/step - loss: 1.7995 - mae: 1.1061 - val_loss: 0.0809 - val_mae: 0.2190 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "1/1 - 0s - 330ms/step - loss: 2.4099 - mae: 1.2586 - val_loss: 0.0847 - val_mae: 0.2243 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "1/1 - 0s - 267ms/step - loss: 2.2626 - mae: 1.3057 - val_loss: 0.0884 - val_mae: 0.2291 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "1/1 - 0s - 303ms/step - loss: 2.5945 - mae: 1.3492 - val_loss: 0.0922 - val_mae: 0.2352 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1/1 - 0s - 287ms/step - loss: 2.0024 - mae: 1.1994 - val_loss: 0.0948 - val_mae: 0.2404 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "1/1 - 1s - 609ms/step - loss: 1.4674 - mae: 0.9782 - val_loss: 0.0982 - val_mae: 0.2442 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "1/1 - 0s - 351ms/step - loss: 1.3955 - mae: 1.0289 - val_loss: 0.1011 - val_mae: 0.2478 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "1/1 - 0s - 331ms/step - loss: 1.1007 - mae: 0.9125 - val_loss: 0.1030 - val_mae: 0.2506 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "1/1 - 0s - 320ms/step - loss: 2.4233 - mae: 1.3108 - val_loss: 0.1053 - val_mae: 0.2537 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1/1 - 0s - 283ms/step - loss: 1.3237 - mae: 1.0158 - val_loss: 0.1076 - val_mae: 0.2568 - learning_rate: 5.0000e-04\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Predicted capex for INDIA in 2027: 0.28957095087622337\n",
      "Predicted capex for INDIA in 2028: 0.30069335876650843\n",
      "Predicted capex for INDIA in 2029: 0.320956429109898\n",
      "\n",
      "Predictions have been successfully saved to 'pred3.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define dataset file paths\n",
    "datasets = {\n",
    "    \"gas_price\": \"gas_price.csv\",\n",
    "    \"rig_count\": \"rig_count.csv\",\n",
    "    \"tam\": \"tam.csv\",\n",
    "    \"capex\": \"capex.csv\",\n",
    "    \"oil_price\": \"oil_price.csv\",\n",
    "    \"education\": \"education.csv\",\n",
    "    \"political_stability\": \"political_stability.csv\",\n",
    "    \"infrastructure_index\": \"infrastructure_index.csv\",\n",
    "    \"health_index\": \"health_index.csv\",\n",
    "    \"gni\": \"gni.xlsx\",\n",
    "    \"gdp\": \"gdp.xlsx\",\n",
    "    \"population\": \"population.xlsx\",\n",
    "    \"country_mapping\": \"country_table(in).csv\" \n",
    "}\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    \"\"\"Loads and processes the dataset.\"\"\"\n",
    "    path = datasets.get(dataset_name)\n",
    "    if not path:\n",
    "        print(f\"Dataset '{dataset_name}' not found in provided paths.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        if path.lower().endswith('.csv'):\n",
    "            df = pd.read_csv(path)\n",
    "        elif path.lower().endswith(('.xlsx', '.xls')):\n",
    "            df = pd.read_excel(path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format for dataset '{dataset_name}'.\")\n",
    "            return None\n",
    "\n",
    "        df[\"country_name\"] = df[\"country_name\"].str.strip().str.upper()\n",
    "        df[\"year\"] = df[\"year\"].astype(int)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "        \n",
    "# ---------- Model Building ----------\n",
    "def build_nn_model(input_dim):\n",
    "    \"\"\"Builds and returns a neural network model.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# ---------- Training Country-Specific Models (rig_count, tam, capex) ----------\n",
    "def train_model_for_country(dataset_name, country):\n",
    "    \"\"\"Trains a neural network model for a specific country and dataset.\"\"\"\n",
    "    df = load_data(dataset_name)\n",
    "    if df is None:\n",
    "        print(f\"Skipping {country}: Unable to load data for {dataset_name}.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    country_df = df[df[\"country_name\"] == country]\n",
    "    if country_df.empty:\n",
    "        print(f\"Skipping {country}: No data available for {dataset_name}.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Assume the target feature is the one that is not 'country_name' or 'year'\n",
    "    target_feature = [col for col in country_df.columns if col in selected_options or col not in [\"country_name\", \"year\"]][0]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # Sort and interpolate missing data\n",
    "    country_df_sorted = country_df.sort_values(\"year\")\n",
    "    country_df_sorted.interpolate(method=\"linear\", inplace=True)\n",
    "\n",
    "    X = country_df_sorted.drop(columns=[\"country_name\", \"year\"])\n",
    "    y = country_df_sorted[[target_feature]]\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    y_scaled = scaler.fit_transform(y)\n",
    "\n",
    "    if len(X_scaled) < 5:\n",
    "        print(f\"Skipping {country}: Not enough data for training in {dataset_name}.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "    model = build_nn_model(input_dim=X_train.shape[1])\n",
    "\n",
    "    # Callbacks for training\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test),\n",
    "              callbacks=[lr_scheduler, early_stopping], verbose=2)\n",
    "\n",
    "    return model, scaler, country_df_sorted, target_feature\n",
    "\n",
    "# ---------- Training Year-Based Models (gas_price, oil_price) ----------\n",
    "def train_nn_model(df, target_feature):\n",
    "    \"\"\"Trains a neural network model using 'year' as the feature.\"\"\"\n",
    "    grouped_df = df.groupby(\"year\").mean(numeric_only=True).reset_index()\n",
    "    X = grouped_df[[\"year\"]].values\n",
    "    y = grouped_df[[target_feature]].values\n",
    "\n",
    "    scaler_X, scaler_y = MinMaxScaler(), MinMaxScaler()\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "    model = build_nn_model(input_dim=1)\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32,\n",
    "              callbacks=[lr_scheduler, early_stopping], verbose=2)\n",
    "\n",
    "    return model, scaler_X, scaler_y\n",
    "\n",
    "# ---------- Prediction Functions ----------\n",
    "def predict_future_by_country(model, scaler, df, target_feature, start_year, num_years, country):\n",
    "    \"\"\"\n",
    "    Predicts future values using a country-specific model.\n",
    "    The returned DataFrame will always use 'year' and 'country_name' as keys.\n",
    "    If the target is 'rig_count', the prediction is rounded to a whole number.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    future_predictions = []\n",
    "    latest_data = df[df[\"year\"] == df[\"year\"].max()]\n",
    "    latest_scaled = scaler.transform(latest_data.drop(columns=[\"country_name\", \"year\"]))\n",
    "\n",
    "    for i in range(num_years):\n",
    "        predicted_scaled = model.predict(latest_scaled, verbose=0)\n",
    "        predicted_value = scaler.inverse_transform(predicted_scaled)[0][0]\n",
    "        trend_factor = np.random.uniform(0.9, 1.1)\n",
    "        predicted_value *= trend_factor\n",
    "\n",
    "        if target_feature == \"rig_count\":\n",
    "            predicted_value = int(round(predicted_value))\n",
    "\n",
    "        prediction_year = start_year + i\n",
    "        future_predictions.append({\n",
    "            \"year\": prediction_year,\n",
    "            \"country_name\": country,\n",
    "            target_feature: predicted_value\n",
    "        })\n",
    "        print(f\"Predicted {target_feature} for {country} in {prediction_year}: {predicted_value}\")\n",
    "\n",
    "    return pd.DataFrame(future_predictions)\n",
    "\n",
    "def predict_prices(model, scaler_X, scaler_y, start, end, countries, feature_name):\n",
    "    \"\"\"\n",
    "    Predicts prices using a year-based model.\n",
    "    Returns a DataFrame with 'year' and 'country_name' columns.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    years = np.arange(start, end + 1).reshape(-1, 1)\n",
    "    years_scaled = scaler_X.transform(years)\n",
    "    preds_scaled = model.predict(years_scaled, verbose=0)\n",
    "    preds = scaler_y.inverse_transform(preds_scaled)\n",
    "\n",
    "    for country in countries:\n",
    "        for year, price in zip(years.flatten(), preds.flatten()):\n",
    "            predictions.append({\n",
    "                \"year\": year,\n",
    "                \"country_name\": country,\n",
    "                feature_name: price\n",
    "            })\n",
    "    return pd.DataFrame(predictions)\n",
    "    \n",
    "\n",
    "# --------------------------------- Main Execution Flow -------------------------------------\n",
    "\n",
    "# Get dataset names (comma separated)\n",
    "valid_datasets = {\"gas_price\", \"oil_price\", \"rig_count\", \"tam\", \"capex\"}\n",
    "dataset_names_input = input(\"\\nEnter what you want to predict (gas_price, oil_price, rig_count, tam, capex): \")\n",
    "dataset_names = []\n",
    "\n",
    "for name in dataset_names_input.split(','):\n",
    "    clean_name = name.strip()\n",
    "    if clean_name in valid_datasets:\n",
    "        dataset_names.append(clean_name)\n",
    "\n",
    "for i, dataset_name in enumerate(datasets.keys(), start=1):\n",
    "    print(f\"{i}. {dataset_name}\")\n",
    "# Menu Display (Select Datasets for Prediction)\n",
    "print(\"\\nAvailable Datasets for Prediction, Here's a catch - If you select all relevant data then predictions is on basis of global indicators else the neural network identify correlation itself adding historical considerations to predict\\n\")\n",
    "print(\"1. Gas Price Prediction: Uses Oil Price, Rig Count, Infrastructure Index, Capex.\")\n",
    "print(\"2. Rig Count Prediction: Uses Rig Count, Infrastructure Index, Capex, Political Stability.\")\n",
    "print(\"3. Oil Price Prediction: Uses Rig Count, GDP Growth, Infrastructure Index, Capex, Political Stability.\")\n",
    "print(\"4. Tam Prediction: Uses Capex, GDP, Population, Health Index, Education, Infrastructure Index.\")\n",
    "print(\"5. Capex Prediction: Uses Oil Price, Gas Price, Rig Count, GDP Growth, Political Stability, Infrastructure Index.\")\n",
    "\n",
    "selected_options = input(\"\\nEnter the numbers corresponding to the datasets you want to predict: \").strip()\n",
    "\n",
    "# Load country mapping to get available countries\n",
    "gas_price_data = load_data(\"gas_price\")\n",
    "if gas_price_data is not None and \"country_name\" in gas_price_data.columns:\n",
    "    available_countries = sorted(gas_price_data[\"country_name\"].unique())\n",
    "    print(\"\\nAvailable countries:\", \", \".join(available_countries))\n",
    "else:\n",
    "    print(\"Gas price data could not be loaded or does not contain country information.\")\n",
    "    available_countries = []\n",
    "\n",
    "\n",
    "# Get countries from user input\n",
    "selected_countries_input = input(\"Enter countries (comma-separated): \")\n",
    "selected_countries = [c.strip().upper() for c in selected_countries_input.split(',')\n",
    "                      if c.strip().upper() in available_countries]\n",
    "\n",
    "# Get prediction year range from user\n",
    "start_year = int(input(\"Enter the start year for predictions: \"))\n",
    "end_year = int(input(\"Enter the end year for predictions: \"))\n",
    "\n",
    "# Train global models for gas and oil prices\n",
    "oil_price_data = load_data(\"oil_price\")\n",
    "gas_model, gas_scaler_X, gas_scaler_y = train_nn_model(gas_price_data, \"gas_price\")\n",
    "oil_model, oil_scaler_X, oil_scaler_y = train_nn_model(oil_price_data, \"oil_price\")\n",
    "\n",
    "# Create a skeleton DataFrame containing all combinations of selected countries and years\n",
    "years = list(range(start_year, end_year + 1))\n",
    "skeleton = pd.DataFrame([(year, country) for country in selected_countries for year in years],\n",
    "                        columns=[\"year\", \"country_name\"])\n",
    "final_predictions = skeleton.copy()\n",
    "\n",
    "# Process each dataset's predictions and merge them into final_predictions\n",
    "for dataset in dataset_names:\n",
    "    if dataset in [\"gas_price\", \"oil_price\"]:\n",
    "        # Global models predict for all countries at once.\n",
    "        if dataset == \"gas_price\" and gas_model:\n",
    "            pred_df = predict_prices(gas_model, gas_scaler_X, gas_scaler_y, start_year, end_year,\n",
    "                                      selected_countries, \"gas_price\")\n",
    "        elif dataset == \"oil_price\" and oil_model:\n",
    "            pred_df = predict_prices(oil_model, oil_scaler_X, oil_scaler_y, start_year, end_year,\n",
    "                                      selected_countries, \"oil_price\")\n",
    "        else:\n",
    "            pred_df = pd.DataFrame()\n",
    "        final_predictions = pd.merge(final_predictions, pred_df, on=[\"year\", \"country_name\"], how=\"left\")\n",
    "    elif dataset in [\"rig_count\", \"tam\", \"capex\"]:\n",
    "        # Country-specific models: train and predict per country.\n",
    "        all_preds = []\n",
    "        for country in selected_countries:\n",
    "            model, scaler, df, target_feature = train_model_for_country(dataset, country)\n",
    "            if model:\n",
    "                num_years = end_year - start_year + 1\n",
    "                pred_df = predict_future_by_country(model, scaler, df, target_feature, start_year, num_years, country)\n",
    "                all_preds.append(pred_df)\n",
    "        if all_preds:\n",
    "            pred_all = pd.concat(all_preds, ignore_index=True)\n",
    "            final_predictions = pd.merge(final_predictions, pred_all, on=[\"year\", \"country_name\"], how=\"left\")\n",
    "\n",
    "def load_country_mapping(file_path):\n",
    "    \"\"\"Loads and preprocesses country mappings from the provided CSV file.\"\"\"\n",
    "    try:\n",
    "        country_mapping = pd.read_csv(file_path)\n",
    "        country_mapping[\"country_name\"] = country_mapping[\"country_name\"].str.strip().str.upper()\n",
    "        return country_mapping\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading country mapping: {e}\")\n",
    "        return None\n",
    "\n",
    "country_mapping = load_country_mapping(datasets[\"country_mapping\"])\n",
    "\n",
    "if country_mapping is not None and \"country_name\" in country_mapping.columns and \"country_id\" in country_mapping.columns:\n",
    "    final_predictions = pd.merge(final_predictions, country_mapping[[\"country_name\", \"country_id\"]], on=\"country_name\", how=\"left\")\n",
    "\n",
    "    # Reorder columns\n",
    "    columns_order = [\"country_id\", \"country_name\", \"year\"] + [col for col in final_predictions.columns if col not in [\"year\", \"country_name\", \"country_id\"]]\n",
    "    final_predictions = final_predictions[columns_order]\n",
    "else:\n",
    "    print(\"Country mapping data is invalid or missing required columns.\")\n",
    "\n",
    "# Save predictions\n",
    "output_file = input(\"\\nEnter the file name to save predictions (e.g., predictions_output.csv): \").strip()\n",
    "output_file += \".csv\" if not output_file.lower().endswith(\".csv\") else \"\"\n",
    "\n",
    "try:\n",
    "    final_predictions.to_csv(output_file, index=False)\n",
    "    print(f\"\\nPredictions have been successfully saved to '{output_file}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving predictions: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
