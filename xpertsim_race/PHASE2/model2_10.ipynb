{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 2019-2023 Shape: (36, 7)\n",
      "Data 2024-2033 Shape: (30, 12)\n",
      "Top 5 Data Flattened Shape: (150, 4)\n",
      "Repurchase Flattened Shape: (30, 4)\n",
      "Merged Data Shape: (750, 6)\n",
      "Top 5 Selected Parameters: ['dividends paid', 'net cash flows used in financing activities', 'gross profit', 'other non-operating income', 'other intangible assets, net']\n",
      "Filtered Data Shape: (200, 6)\n",
      "Data Pivoted Columns: []\n",
      "Data Pivoted Shape: (0, 0)\n",
      "Target Shape: (6,)\n",
      "Aligned Data Shape: (0, 0)\n",
      "Aligned Target Shape: (0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14556\\117383222.py:48: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group['Value'].corr(group['Repurchase']))\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14556\\117383222.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['Value'] = pd.to_numeric(filtered_data['Value'], errors='coerce')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14556\\117383222.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data.dropna(subset=['Value'], inplace=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid training data after preprocessing. Check filtering steps.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14556\\117383222.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;31m# Ensure data is not empty before splitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata_pivoted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No valid training data after preprocessing. Check filtering steps.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;31m# Fill missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No valid training data after preprocessing. Check filtering steps."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "# Load datasets (replace file paths with actual locations of your datasets)\n",
    "data_2019_2023 = pd.read_excel(\"top 5+ stock 2019-2023.xlsx\")\n",
    "data_2024_2033 = pd.read_excel(\"top 5 2024_2033.xlsx\")\n",
    "\n",
    "# Ensure column names are stripped of spaces\n",
    "data_2019_2023.columns = data_2019_2023.columns.str.strip()\n",
    "data_2024_2033.columns = data_2024_2033.columns.str.strip()\n",
    "\n",
    "# Extract relevant data\n",
    "repurchase_data = data_2019_2023[data_2019_2023['Parameters'].str.strip() == 'repurchase of common stock']\n",
    "top_5_parameters = data_2019_2023[data_2019_2023['Parameters'].str.strip() != 'repurchase of common stock']\n",
    "\n",
    "# Debugging: Print dataset shapes\n",
    "print(\"Data 2019-2023 Shape:\", data_2019_2023.shape)\n",
    "print(\"Data 2024-2033 Shape:\", data_2024_2033.shape)\n",
    "\n",
    "# Reshape and align datasets\n",
    "# Melt datasets to align by year\n",
    "top_5_data_flattened = top_5_parameters.melt(id_vars=['Company_name', 'Parameters'], \n",
    "                                             var_name='Year', \n",
    "                                             value_name='Value')\n",
    "repurchase_flattened = repurchase_data.melt(id_vars=['Company_name', 'Parameters'], \n",
    "                                            var_name='Year', \n",
    "                                            value_name='Repurchase')\n",
    "\n",
    "# Debugging: Print dataset shapes after melting\n",
    "print(\"Top 5 Data Flattened Shape:\", top_5_data_flattened.shape)\n",
    "print(\"Repurchase Flattened Shape:\", repurchase_flattened.shape)\n",
    "\n",
    "# Merge datasets\n",
    "merged_data = pd.merge(top_5_data_flattened, repurchase_flattened, on=['Company_name', 'Year'], how='inner')\n",
    "merged_data.rename(columns={'Parameters_x': 'Parameters', 'Parameters_y': 'Repurchase_Parameter'}, inplace=True)\n",
    "\n",
    "# Debugging: Check merge success\n",
    "print(\"Merged Data Shape:\", merged_data.shape)\n",
    "\n",
    "# Compute top 5 parameters by correlation\n",
    "if not merged_data.empty:\n",
    "    correlations = (\n",
    "        merged_data.groupby('Parameters')\n",
    "        .apply(lambda group: group['Value'].corr(group['Repurchase']))\n",
    "        .dropna()  # Remove NaN correlations\n",
    "        .sort_values(ascending=False)\n",
    "        .head(5)\n",
    "    )\n",
    "    top_5_selected_params = correlations.index.tolist()\n",
    "else:\n",
    "    top_5_selected_params = []\n",
    "\n",
    "# Debugging: Print correlation results\n",
    "print(\"Top 5 Selected Parameters:\", top_5_selected_params)\n",
    "\n",
    "# Ensure there are selected parameters\n",
    "if len(top_5_selected_params) == 0:\n",
    "    raise ValueError(\"No parameters selected based on correlation. Check data consistency.\")\n",
    "\n",
    "# Filter data for top 5 parameters\n",
    "filtered_data = merged_data[merged_data['Parameters'].isin(top_5_selected_params)]\n",
    "\n",
    "# Debugging: Check filtered data\n",
    "print(\"Filtered Data Shape:\", filtered_data.shape)\n",
    "\n",
    "# Ensure numeric values before pivoting\n",
    "filtered_data['Value'] = pd.to_numeric(filtered_data['Value'], errors='coerce')\n",
    "filtered_data.dropna(subset=['Value'], inplace=True)\n",
    "\n",
    "# Prepare features and target\n",
    "data_pivoted = filtered_data.pivot_table(index=['Company_name', 'Year'], \n",
    "                                         columns='Parameters', \n",
    "                                         values='Value')\n",
    "\n",
    "# Debugging: Check data pivoted\n",
    "print(\"Data Pivoted Columns:\", data_pivoted.columns.tolist())\n",
    "print(\"Data Pivoted Shape:\", data_pivoted.shape)\n",
    "\n",
    "# Prepare target variable\n",
    "target = repurchase_flattened.drop_duplicates(subset=['Company_name', 'Year']).set_index(['Company_name', 'Year'])['Repurchase']\n",
    "\n",
    "# Debugging: Check target\n",
    "print(\"Target Shape:\", target.shape)\n",
    "\n",
    "# Align features and target\n",
    "data_pivoted, target = data_pivoted.align(target, join='inner', axis=0)\n",
    "\n",
    "# Debugging: Check shapes after alignment\n",
    "print(\"Aligned Data Shape:\", data_pivoted.shape)\n",
    "print(\"Aligned Target Shape:\", target.shape)\n",
    "\n",
    "# Ensure data is not empty before splitting\n",
    "if data_pivoted.shape[0] == 0 or target.shape[0] == 0:\n",
    "    raise ValueError(\"No valid training data after preprocessing. Check filtering steps.\")\n",
    "\n",
    "# Fill missing values\n",
    "data_pivoted.fillna(0, inplace=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_pivoted, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build an enhanced neural network model\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    BatchNormalization(),  # Normalize activations\n",
    "    Dropout(0.3),  # Add dropout for regularization\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model with Huber loss\n",
    "model.compile(optimizer='adam', loss=Huber(), metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=500, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")\n",
    "\n",
    "# Save predictions to CSV\n",
    "forecast_features.to_csv(\"predicted_repurchase_2024_2033.csv\", index=False)\n",
    "print(\"Predictions saved to 'predicted_2024_2033.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14556\\507644556.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group['Value'].corr(group['Repurchase']))\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 1187.0000 - mae: 1187.5000 - val_loss: 29.6248 - val_mae: 30.0000\n",
      "Epoch 2/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - loss: 1186.9990 - mae: 1187.4990 - val_loss: 29.6245 - val_mae: 30.0000\n",
      "Epoch 3/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 515ms/step - loss: 1186.9980 - mae: 1187.4980 - val_loss: 29.6243 - val_mae: 30.0000\n",
      "Epoch 4/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - loss: 1186.9971 - mae: 1187.4971 - val_loss: 29.6240 - val_mae: 30.0000\n",
      "Epoch 5/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 1186.9961 - mae: 1187.4961 - val_loss: 29.6238 - val_mae: 30.0000\n",
      "Epoch 6/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 1186.9951 - mae: 1187.4951 - val_loss: 29.6235 - val_mae: 30.0000\n",
      "Epoch 7/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - loss: 1186.9941 - mae: 1187.4941 - val_loss: 29.6233 - val_mae: 30.0000\n",
      "Epoch 8/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - loss: 1186.9930 - mae: 1187.4930 - val_loss: 29.6230 - val_mae: 30.0000\n",
      "Epoch 9/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - loss: 1186.9921 - mae: 1187.4921 - val_loss: 29.6228 - val_mae: 30.0000\n",
      "Epoch 10/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - loss: 1186.9911 - mae: 1187.4911 - val_loss: 29.6225 - val_mae: 30.0000\n",
      "Epoch 11/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - loss: 1186.9901 - mae: 1187.4901 - val_loss: 29.6223 - val_mae: 30.0000\n",
      "Epoch 12/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - loss: 1186.9889 - mae: 1187.4889 - val_loss: 29.6220 - val_mae: 30.0000\n",
      "Epoch 13/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - loss: 1186.9879 - mae: 1187.4879 - val_loss: 29.6218 - val_mae: 30.0000\n",
      "Epoch 14/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - loss: 1186.9869 - mae: 1187.4869 - val_loss: 29.6215 - val_mae: 30.0000\n",
      "Epoch 15/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433ms/step - loss: 1186.9860 - mae: 1187.4860 - val_loss: 29.6213 - val_mae: 30.0000\n",
      "Epoch 16/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step - loss: 1186.9849 - mae: 1187.4849 - val_loss: 29.6210 - val_mae: 30.0000\n",
      "Epoch 17/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 1186.9840 - mae: 1187.4840 - val_loss: 29.6208 - val_mae: 30.0000\n",
      "Epoch 18/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 1186.9829 - mae: 1187.4829 - val_loss: 29.6205 - val_mae: 30.0000\n",
      "Epoch 19/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1186.9819 - mae: 1187.4819 - val_loss: 29.6203 - val_mae: 30.0000\n",
      "Epoch 20/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step - loss: 1186.9810 - mae: 1187.4810 - val_loss: 29.6201 - val_mae: 30.0000\n",
      "Epoch 21/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 1186.9800 - mae: 1187.4800 - val_loss: 29.6198 - val_mae: 30.0000\n",
      "Epoch 22/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 1186.9790 - mae: 1187.4790 - val_loss: 29.6196 - val_mae: 30.0000\n",
      "Epoch 23/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 1186.9780 - mae: 1187.4780 - val_loss: 29.6193 - val_mae: 30.0000\n",
      "Epoch 24/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - loss: 1186.9771 - mae: 1187.4771 - val_loss: 29.6191 - val_mae: 30.0000\n",
      "Epoch 25/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 1186.9761 - mae: 1187.4761 - val_loss: 29.6188 - val_mae: 30.0000\n",
      "Epoch 26/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - loss: 1186.9751 - mae: 1187.4751 - val_loss: 29.6186 - val_mae: 30.0000\n",
      "Epoch 27/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 561ms/step - loss: 1186.9741 - mae: 1187.4741 - val_loss: 29.6183 - val_mae: 30.0000\n",
      "Epoch 28/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - loss: 1186.9730 - mae: 1187.4730 - val_loss: 29.6181 - val_mae: 30.0000\n",
      "Epoch 29/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - loss: 1186.9720 - mae: 1187.4720 - val_loss: 29.6179 - val_mae: 30.0000\n",
      "Epoch 30/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - loss: 1186.9711 - mae: 1187.4711 - val_loss: 29.6176 - val_mae: 30.0000\n",
      "Epoch 31/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 1186.9701 - mae: 1187.4701 - val_loss: 29.6174 - val_mae: 30.0000\n",
      "Epoch 32/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 1186.9691 - mae: 1187.4691 - val_loss: 29.6171 - val_mae: 30.0000\n",
      "Epoch 33/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - loss: 1186.9679 - mae: 1187.4679 - val_loss: 29.6169 - val_mae: 30.0000\n",
      "Epoch 34/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 1186.9669 - mae: 1187.4669 - val_loss: 29.6166 - val_mae: 30.0000\n",
      "Epoch 35/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - loss: 1186.9659 - mae: 1187.4659 - val_loss: 29.6164 - val_mae: 30.0000\n",
      "Epoch 36/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - loss: 1186.9650 - mae: 1187.4650 - val_loss: 29.6162 - val_mae: 30.0000\n",
      "Epoch 37/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1186.9640 - mae: 1187.4640 - val_loss: 29.6159 - val_mae: 30.0000\n",
      "Epoch 38/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - loss: 1186.9629 - mae: 1187.4629 - val_loss: 29.6157 - val_mae: 30.0000\n",
      "Epoch 39/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1186.9619 - mae: 1187.4619 - val_loss: 29.6154 - val_mae: 30.0000\n",
      "Epoch 40/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 1186.9609 - mae: 1187.4609 - val_loss: 29.6152 - val_mae: 30.0000\n",
      "Epoch 41/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 1186.9600 - mae: 1187.4600 - val_loss: 29.6150 - val_mae: 30.0000\n",
      "Epoch 42/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433ms/step - loss: 1186.9590 - mae: 1187.4590 - val_loss: 29.6147 - val_mae: 30.0000\n",
      "Epoch 43/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 1186.9580 - mae: 1187.4580 - val_loss: 29.6145 - val_mae: 30.0000\n",
      "Epoch 44/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - loss: 1186.9570 - mae: 1187.4570 - val_loss: 29.6142 - val_mae: 30.0000\n",
      "Epoch 45/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - loss: 1186.9561 - mae: 1187.4561 - val_loss: 29.6140 - val_mae: 30.0000\n",
      "Epoch 46/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1186.9551 - mae: 1187.4551 - val_loss: 29.6138 - val_mae: 30.0000\n",
      "Epoch 47/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - loss: 1186.9541 - mae: 1187.4541 - val_loss: 29.6135 - val_mae: 30.0000\n",
      "Epoch 48/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - loss: 1186.9531 - mae: 1187.4531 - val_loss: 29.6133 - val_mae: 30.0000\n",
      "Epoch 49/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 1186.9521 - mae: 1187.4521 - val_loss: 29.6131 - val_mae: 30.0000\n",
      "Epoch 50/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - loss: 1186.9510 - mae: 1187.4510 - val_loss: 29.6128 - val_mae: 30.0000\n",
      "Epoch 51/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 1186.9501 - mae: 1187.4501 - val_loss: 29.6126 - val_mae: 30.0000\n",
      "Epoch 52/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - loss: 1186.9491 - mae: 1187.4491 - val_loss: 29.6123 - val_mae: 30.0000\n",
      "Epoch 53/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - loss: 1186.9481 - mae: 1187.4481 - val_loss: 29.6121 - val_mae: 30.0000\n",
      "Epoch 54/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - loss: 1186.9469 - mae: 1187.4469 - val_loss: 29.6119 - val_mae: 30.0000\n",
      "Epoch 55/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - loss: 1186.9459 - mae: 1187.4459 - val_loss: 29.6116 - val_mae: 30.0000\n",
      "Epoch 56/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - loss: 1186.9449 - mae: 1187.4449 - val_loss: 29.6114 - val_mae: 30.0000\n",
      "Epoch 57/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - loss: 1186.9440 - mae: 1187.4440 - val_loss: 29.6112 - val_mae: 30.0000\n",
      "Epoch 58/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - loss: 1186.9430 - mae: 1187.4430 - val_loss: 29.6109 - val_mae: 30.0000\n",
      "Epoch 59/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - loss: 1186.9419 - mae: 1187.4419 - val_loss: 29.6107 - val_mae: 30.0000\n",
      "Epoch 60/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 1186.9409 - mae: 1187.4409 - val_loss: 29.6105 - val_mae: 30.0000\n",
      "Epoch 61/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - loss: 1186.9399 - mae: 1187.4399 - val_loss: 29.6102 - val_mae: 30.0000\n",
      "Epoch 62/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 1186.9390 - mae: 1187.4390 - val_loss: 29.6100 - val_mae: 30.0000\n",
      "Epoch 63/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 1186.9380 - mae: 1187.4380 - val_loss: 29.6097 - val_mae: 30.0000\n",
      "Epoch 64/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - loss: 1186.9370 - mae: 1187.4370 - val_loss: 29.6095 - val_mae: 30.0000\n",
      "Epoch 65/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - loss: 1186.9360 - mae: 1187.4360 - val_loss: 29.6093 - val_mae: 30.0000\n",
      "Epoch 66/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 1186.9351 - mae: 1187.4351 - val_loss: 29.6090 - val_mae: 30.0000\n",
      "Epoch 67/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 1186.9341 - mae: 1187.4341 - val_loss: 29.6088 - val_mae: 30.0000\n",
      "Epoch 68/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 1186.9331 - mae: 1187.4331 - val_loss: 29.6086 - val_mae: 30.0000\n",
      "Epoch 69/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 1186.9321 - mae: 1187.4321 - val_loss: 29.6083 - val_mae: 30.0000\n",
      "Epoch 70/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 1186.9310 - mae: 1187.4310 - val_loss: 29.6081 - val_mae: 30.0000\n",
      "Epoch 71/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1186.9301 - mae: 1187.4301 - val_loss: 29.6079 - val_mae: 30.0000\n",
      "Epoch 72/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - loss: 1186.9291 - mae: 1187.4291 - val_loss: 29.6076 - val_mae: 30.0000\n",
      "Epoch 73/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435ms/step - loss: 1186.9281 - mae: 1187.4281 - val_loss: 29.6074 - val_mae: 30.0000\n",
      "Epoch 74/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - loss: 1186.9269 - mae: 1187.4269 - val_loss: 29.6072 - val_mae: 30.0000\n",
      "Epoch 75/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - loss: 1186.9259 - mae: 1187.4259 - val_loss: 29.6070 - val_mae: 30.0000\n",
      "Epoch 76/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - loss: 1186.9249 - mae: 1187.4249 - val_loss: 29.6067 - val_mae: 30.0000\n",
      "Epoch 77/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 1186.9240 - mae: 1187.4240 - val_loss: 29.6065 - val_mae: 30.0000\n",
      "Epoch 78/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - loss: 1186.9230 - mae: 1187.4230 - val_loss: 29.6063 - val_mae: 30.0000\n",
      "Epoch 79/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - loss: 1186.9219 - mae: 1187.4219 - val_loss: 29.6060 - val_mae: 30.0000\n",
      "Epoch 80/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - loss: 1186.9209 - mae: 1187.4209 - val_loss: 29.6058 - val_mae: 30.0000\n",
      "Epoch 81/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - loss: 1186.9199 - mae: 1187.4199 - val_loss: 29.6056 - val_mae: 30.0000\n",
      "Epoch 82/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - loss: 1186.9189 - mae: 1187.4189 - val_loss: 29.6053 - val_mae: 30.0000\n",
      "Epoch 83/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - loss: 1186.9180 - mae: 1187.4180 - val_loss: 29.6051 - val_mae: 30.0000\n",
      "Epoch 84/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 1186.9170 - mae: 1187.4170 - val_loss: 29.6049 - val_mae: 30.0000\n",
      "Epoch 85/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 1186.9160 - mae: 1187.4160 - val_loss: 29.6047 - val_mae: 30.0000\n",
      "Epoch 86/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 1186.9150 - mae: 1187.4150 - val_loss: 29.6044 - val_mae: 30.0000\n",
      "Epoch 87/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426ms/step - loss: 1186.9141 - mae: 1187.4141 - val_loss: 29.6042 - val_mae: 30.0000\n",
      "Epoch 88/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 1186.9131 - mae: 1187.4131 - val_loss: 29.6040 - val_mae: 30.0000\n",
      "Epoch 89/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1186.9121 - mae: 1187.4121 - val_loss: 29.6037 - val_mae: 30.0000\n",
      "Epoch 90/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1186.9111 - mae: 1187.4111 - val_loss: 29.6035 - val_mae: 30.0000\n",
      "Epoch 91/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - loss: 1186.9100 - mae: 1187.4100 - val_loss: 29.6033 - val_mae: 30.0000\n",
      "Epoch 92/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 512ms/step - loss: 1186.9091 - mae: 1187.4091 - val_loss: 29.6031 - val_mae: 30.0000\n",
      "Epoch 93/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 561ms/step - loss: 1186.9081 - mae: 1187.4081 - val_loss: 29.6028 - val_mae: 30.0000\n",
      "Epoch 94/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 1186.9071 - mae: 1187.4071 - val_loss: 29.6026 - val_mae: 30.0000\n",
      "Epoch 95/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 1186.9059 - mae: 1187.4059 - val_loss: 29.6024 - val_mae: 30.0000\n",
      "Epoch 96/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - loss: 1186.9049 - mae: 1187.4049 - val_loss: 29.6022 - val_mae: 30.0000\n",
      "Epoch 97/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - loss: 1186.9039 - mae: 1187.4039 - val_loss: 29.6019 - val_mae: 30.0000\n",
      "Epoch 98/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - loss: 1186.9030 - mae: 1187.4030 - val_loss: 29.6017 - val_mae: 30.0000\n",
      "Epoch 99/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step - loss: 1186.9020 - mae: 1187.4020 - val_loss: 29.6015 - val_mae: 30.0000\n",
      "Epoch 100/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 1186.9009 - mae: 1187.4009 - val_loss: 29.6012 - val_mae: 30.0000\n",
      "Epoch 101/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - loss: 1186.8999 - mae: 1187.3999 - val_loss: 29.6010 - val_mae: 30.0000\n",
      "Epoch 102/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - loss: 1186.8989 - mae: 1187.3989 - val_loss: 29.6008 - val_mae: 30.0000\n",
      "Epoch 103/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 1186.8979 - mae: 1187.3979 - val_loss: 29.6006 - val_mae: 30.0000\n",
      "Epoch 104/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1186.8970 - mae: 1187.3970 - val_loss: 29.6004 - val_mae: 30.0000\n",
      "Epoch 105/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step - loss: 1186.8960 - mae: 1187.3960 - val_loss: 29.6001 - val_mae: 30.0000\n",
      "Epoch 106/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - loss: 1186.8950 - mae: 1187.3950 - val_loss: 29.5999 - val_mae: 30.0000\n",
      "Epoch 107/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 1186.8940 - mae: 1187.3940 - val_loss: 29.5997 - val_mae: 30.0000\n",
      "Epoch 108/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 1186.8929 - mae: 1187.3929 - val_loss: 29.5995 - val_mae: 30.0000\n",
      "Epoch 109/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - loss: 1186.8921 - mae: 1187.3921 - val_loss: 29.5992 - val_mae: 30.0000\n",
      "Epoch 110/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475ms/step - loss: 1186.8911 - mae: 1187.3911 - val_loss: 29.5990 - val_mae: 30.0000\n",
      "Epoch 111/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 1186.8900 - mae: 1187.3900 - val_loss: 29.5988 - val_mae: 30.0000\n",
      "Epoch 112/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - loss: 1186.8890 - mae: 1187.3890 - val_loss: 29.5986 - val_mae: 30.0000\n",
      "Epoch 113/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 1186.8881 - mae: 1187.3881 - val_loss: 29.5983 - val_mae: 30.0000\n",
      "Epoch 114/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 1186.8871 - mae: 1187.3871 - val_loss: 29.5981 - val_mae: 30.0000\n",
      "Epoch 115/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 1186.8861 - mae: 1187.3861 - val_loss: 29.5979 - val_mae: 30.0000\n",
      "Epoch 116/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1186.8849 - mae: 1187.3849 - val_loss: 29.5977 - val_mae: 30.0000\n",
      "Epoch 117/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 1186.8839 - mae: 1187.3839 - val_loss: 29.5975 - val_mae: 30.0000\n",
      "Epoch 118/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 1186.8829 - mae: 1187.3829 - val_loss: 29.5972 - val_mae: 30.0000\n",
      "Epoch 119/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 1186.8820 - mae: 1187.3820 - val_loss: 29.5970 - val_mae: 30.0000\n",
      "Epoch 120/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 1186.8810 - mae: 1187.3810 - val_loss: 29.5968 - val_mae: 30.0000\n",
      "Epoch 121/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 1186.8800 - mae: 1187.3800 - val_loss: 29.5966 - val_mae: 30.0000\n",
      "Epoch 122/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - loss: 1186.8789 - mae: 1187.3789 - val_loss: 29.5964 - val_mae: 30.0000\n",
      "Epoch 123/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - loss: 1186.8779 - mae: 1187.3779 - val_loss: 29.5961 - val_mae: 30.0000\n",
      "Epoch 124/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 1186.8770 - mae: 1187.3770 - val_loss: 29.5959 - val_mae: 30.0000\n",
      "Epoch 125/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 1186.8760 - mae: 1187.3760 - val_loss: 29.5957 - val_mae: 30.0000\n",
      "Epoch 126/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 1186.8750 - mae: 1187.3750 - val_loss: 29.5955 - val_mae: 30.0000\n",
      "Epoch 127/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 1186.8740 - mae: 1187.3740 - val_loss: 29.5953 - val_mae: 30.0000\n",
      "Epoch 128/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380ms/step - loss: 1186.8730 - mae: 1187.3730 - val_loss: 29.5950 - val_mae: 30.0000\n",
      "Epoch 129/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - loss: 1186.8721 - mae: 1187.3721 - val_loss: 29.5948 - val_mae: 30.0000\n",
      "Epoch 130/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - loss: 1186.8711 - mae: 1187.3711 - val_loss: 29.5946 - val_mae: 30.0000\n",
      "Epoch 131/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 537ms/step - loss: 1186.8700 - mae: 1187.3700 - val_loss: 29.5944 - val_mae: 30.0000\n",
      "Epoch 132/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - loss: 1186.8691 - mae: 1187.3691 - val_loss: 29.5942 - val_mae: 30.0000\n",
      "Epoch 133/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - loss: 1186.8680 - mae: 1187.3680 - val_loss: 29.5940 - val_mae: 30.0000\n",
      "Epoch 134/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step - loss: 1186.8671 - mae: 1187.3671 - val_loss: 29.5937 - val_mae: 30.0000\n",
      "Epoch 135/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 1186.8661 - mae: 1187.3661 - val_loss: 29.5935 - val_mae: 30.0000\n",
      "Epoch 136/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - loss: 1186.8651 - mae: 1187.3651 - val_loss: 29.5933 - val_mae: 30.0000\n",
      "Epoch 137/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step - loss: 1186.8639 - mae: 1187.3639 - val_loss: 29.5931 - val_mae: 30.0000\n",
      "Epoch 138/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - loss: 1186.8629 - mae: 1187.3629 - val_loss: 29.5929 - val_mae: 30.0000\n",
      "Epoch 139/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - loss: 1186.8619 - mae: 1187.3619 - val_loss: 29.5927 - val_mae: 30.0000\n",
      "Epoch 140/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - loss: 1186.8610 - mae: 1187.3610 - val_loss: 29.5924 - val_mae: 30.0000\n",
      "Epoch 141/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - loss: 1186.8599 - mae: 1187.3599 - val_loss: 29.5922 - val_mae: 30.0000\n",
      "Epoch 142/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - loss: 1186.8589 - mae: 1187.3589 - val_loss: 29.5920 - val_mae: 30.0000\n",
      "Epoch 143/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - loss: 1186.8579 - mae: 1187.3579 - val_loss: 29.5918 - val_mae: 30.0000\n",
      "Epoch 144/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - loss: 1186.8569 - mae: 1187.3569 - val_loss: 29.5916 - val_mae: 30.0000\n",
      "Epoch 145/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - loss: 1186.8560 - mae: 1187.3560 - val_loss: 29.5914 - val_mae: 30.0000\n",
      "Epoch 146/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - loss: 1186.8550 - mae: 1187.3550 - val_loss: 29.5912 - val_mae: 30.0000\n",
      "Epoch 147/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 620ms/step - loss: 1186.8540 - mae: 1187.3540 - val_loss: 29.5910 - val_mae: 30.0000\n",
      "Epoch 148/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565ms/step - loss: 1186.8529 - mae: 1187.3529 - val_loss: 29.5907 - val_mae: 30.0000\n",
      "Epoch 149/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - loss: 1186.8521 - mae: 1187.3521 - val_loss: 29.5905 - val_mae: 30.0000\n",
      "Epoch 150/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 1186.8511 - mae: 1187.3511 - val_loss: 29.5903 - val_mae: 30.0000\n",
      "Epoch 151/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - loss: 1186.8501 - mae: 1187.3501 - val_loss: 29.5901 - val_mae: 30.0000\n",
      "Epoch 152/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - loss: 1186.8491 - mae: 1187.3491 - val_loss: 29.5899 - val_mae: 30.0000\n",
      "Epoch 153/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - loss: 1186.8481 - mae: 1187.3481 - val_loss: 29.5897 - val_mae: 30.0000\n",
      "Epoch 154/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 1186.8470 - mae: 1187.3470 - val_loss: 29.5895 - val_mae: 30.0000\n",
      "Epoch 155/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - loss: 1186.8461 - mae: 1187.3461 - val_loss: 29.5893 - val_mae: 30.0000\n",
      "Epoch 156/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - loss: 1186.8451 - mae: 1187.3451 - val_loss: 29.5890 - val_mae: 30.0000\n",
      "Epoch 157/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 1186.8441 - mae: 1187.3441 - val_loss: 29.5888 - val_mae: 30.0000\n",
      "Epoch 158/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - loss: 1186.8429 - mae: 1187.3429 - val_loss: 29.5886 - val_mae: 30.0000\n",
      "Epoch 159/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - loss: 1186.8419 - mae: 1187.3419 - val_loss: 29.5884 - val_mae: 30.0000\n",
      "Epoch 160/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - loss: 1186.8409 - mae: 1187.3409 - val_loss: 29.5882 - val_mae: 30.0000\n",
      "Epoch 161/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - loss: 1186.8400 - mae: 1187.3400 - val_loss: 29.5880 - val_mae: 30.0000\n",
      "Epoch 162/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - loss: 1186.8390 - mae: 1187.3390 - val_loss: 29.5878 - val_mae: 30.0000\n",
      "Epoch 163/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - loss: 1186.8379 - mae: 1187.3379 - val_loss: 29.5876 - val_mae: 30.0000\n",
      "Epoch 164/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 1186.8370 - mae: 1187.3370 - val_loss: 29.5874 - val_mae: 30.0000\n",
      "Epoch 165/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 1186.8359 - mae: 1187.3359 - val_loss: 29.5872 - val_mae: 30.0000\n",
      "Epoch 166/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - loss: 1186.8350 - mae: 1187.3350 - val_loss: 29.5869 - val_mae: 30.0000\n",
      "Epoch 167/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - loss: 1186.8340 - mae: 1187.3340 - val_loss: 29.5867 - val_mae: 30.0000\n",
      "Epoch 168/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 1186.8330 - mae: 1187.3330 - val_loss: 29.5865 - val_mae: 30.0000\n",
      "Epoch 169/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - loss: 1186.8320 - mae: 1187.3320 - val_loss: 29.5863 - val_mae: 30.0000\n",
      "Epoch 170/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 1186.8311 - mae: 1187.3311 - val_loss: 29.5861 - val_mae: 30.0000\n",
      "Epoch 171/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 1186.8301 - mae: 1187.3301 - val_loss: 29.5859 - val_mae: 30.0000\n",
      "Epoch 172/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - loss: 1186.8290 - mae: 1187.3290 - val_loss: 29.5857 - val_mae: 30.0000\n",
      "Epoch 173/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - loss: 1186.8280 - mae: 1187.3280 - val_loss: 29.5855 - val_mae: 30.0000\n",
      "Epoch 174/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - loss: 1186.8270 - mae: 1187.3270 - val_loss: 29.5853 - val_mae: 30.0000\n",
      "Epoch 175/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - loss: 1186.8260 - mae: 1187.3260 - val_loss: 29.5851 - val_mae: 30.0000\n",
      "Epoch 176/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - loss: 1186.8251 - mae: 1187.3251 - val_loss: 29.5849 - val_mae: 30.0000\n",
      "Epoch 177/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - loss: 1186.8241 - mae: 1187.3241 - val_loss: 29.5847 - val_mae: 30.0000\n",
      "Epoch 178/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443ms/step - loss: 1186.8231 - mae: 1187.3231 - val_loss: 29.5845 - val_mae: 30.0000\n",
      "Epoch 179/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - loss: 1186.8219 - mae: 1187.3219 - val_loss: 29.5843 - val_mae: 30.0000\n",
      "Epoch 180/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - loss: 1186.8209 - mae: 1187.3209 - val_loss: 29.5840 - val_mae: 30.0000\n",
      "Epoch 181/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - loss: 1186.8199 - mae: 1187.3199 - val_loss: 29.5838 - val_mae: 30.0000\n",
      "Epoch 182/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - loss: 1186.8190 - mae: 1187.3190 - val_loss: 29.5836 - val_mae: 30.0000\n",
      "Epoch 183/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step - loss: 1186.8179 - mae: 1187.3179 - val_loss: 29.5834 - val_mae: 30.0000\n",
      "Epoch 184/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - loss: 1186.8169 - mae: 1187.3169 - val_loss: 29.5832 - val_mae: 30.0000\n",
      "Epoch 185/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - loss: 1186.8159 - mae: 1187.3159 - val_loss: 29.5830 - val_mae: 30.0000\n",
      "Epoch 186/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 1186.8149 - mae: 1187.3149 - val_loss: 29.5828 - val_mae: 30.0000\n",
      "Epoch 187/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - loss: 1186.8140 - mae: 1187.3140 - val_loss: 29.5826 - val_mae: 30.0000\n",
      "Epoch 188/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 1186.8130 - mae: 1187.3130 - val_loss: 29.5824 - val_mae: 30.0000\n",
      "Epoch 189/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - loss: 1186.8120 - mae: 1187.3120 - val_loss: 29.5822 - val_mae: 30.0000\n",
      "Epoch 190/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - loss: 1186.8110 - mae: 1187.3110 - val_loss: 29.5820 - val_mae: 30.0000\n",
      "Epoch 191/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - loss: 1186.8101 - mae: 1187.3101 - val_loss: 29.5818 - val_mae: 30.0000\n",
      "Epoch 192/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - loss: 1186.8091 - mae: 1187.3091 - val_loss: 29.5816 - val_mae: 30.0000\n",
      "Epoch 193/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 1186.8081 - mae: 1187.3081 - val_loss: 29.5814 - val_mae: 30.0000\n",
      "Epoch 194/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - loss: 1186.8070 - mae: 1187.3070 - val_loss: 29.5812 - val_mae: 30.0000\n",
      "Epoch 195/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step - loss: 1186.8062 - mae: 1187.3062 - val_loss: 29.5810 - val_mae: 30.0000\n",
      "Epoch 196/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - loss: 1186.8051 - mae: 1187.3051 - val_loss: 29.5808 - val_mae: 30.0000\n",
      "Epoch 197/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - loss: 1186.8041 - mae: 1187.3041 - val_loss: 29.5806 - val_mae: 30.0000\n",
      "Epoch 198/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - loss: 1186.8031 - mae: 1187.3031 - val_loss: 29.5804 - val_mae: 30.0000\n",
      "Epoch 199/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - loss: 1186.8019 - mae: 1187.3019 - val_loss: 29.5802 - val_mae: 30.0000\n",
      "Epoch 200/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 1186.8009 - mae: 1187.3009 - val_loss: 29.5800 - val_mae: 30.0000\n",
      "Epoch 201/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 1186.7999 - mae: 1187.2999 - val_loss: 29.5798 - val_mae: 30.0000\n",
      "Epoch 202/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 1186.7990 - mae: 1187.2990 - val_loss: 29.5796 - val_mae: 30.0000\n",
      "Epoch 203/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 1186.7980 - mae: 1187.2980 - val_loss: 29.5794 - val_mae: 30.0000\n",
      "Epoch 204/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - loss: 1186.7969 - mae: 1187.2969 - val_loss: 29.5792 - val_mae: 30.0000\n",
      "Epoch 205/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - loss: 1186.7959 - mae: 1187.2959 - val_loss: 29.5790 - val_mae: 30.0000\n",
      "Epoch 206/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - loss: 1186.7949 - mae: 1187.2949 - val_loss: 29.5788 - val_mae: 30.0000\n",
      "Epoch 207/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - loss: 1186.7939 - mae: 1187.2939 - val_loss: 29.5786 - val_mae: 30.0000\n",
      "Epoch 208/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - loss: 1186.7930 - mae: 1187.2930 - val_loss: 29.5784 - val_mae: 30.0000\n",
      "Epoch 209/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 1186.7920 - mae: 1187.2920 - val_loss: 29.5782 - val_mae: 30.0000\n",
      "Epoch 210/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - loss: 1186.7910 - mae: 1187.2910 - val_loss: 29.5780 - val_mae: 30.0000\n",
      "Epoch 211/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - loss: 1186.7900 - mae: 1187.2900 - val_loss: 29.5778 - val_mae: 30.0000\n",
      "Epoch 212/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 1186.7891 - mae: 1187.2891 - val_loss: 29.5776 - val_mae: 30.0000\n",
      "Epoch 213/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - loss: 1186.7881 - mae: 1187.2881 - val_loss: 29.5774 - val_mae: 30.0000\n",
      "Epoch 214/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step - loss: 1186.7871 - mae: 1187.2871 - val_loss: 29.5772 - val_mae: 30.0000\n",
      "Epoch 215/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - loss: 1186.7861 - mae: 1187.2861 - val_loss: 29.5770 - val_mae: 30.0000\n",
      "Epoch 216/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - loss: 1186.7852 - mae: 1187.2852 - val_loss: 29.5768 - val_mae: 30.0000\n",
      "Epoch 217/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - loss: 1186.7841 - mae: 1187.2841 - val_loss: 29.5766 - val_mae: 30.0000\n",
      "Epoch 218/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - loss: 1186.7831 - mae: 1187.2831 - val_loss: 29.5764 - val_mae: 30.0000\n",
      "Epoch 219/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 1186.7821 - mae: 1187.2821 - val_loss: 29.5762 - val_mae: 30.0000\n",
      "Epoch 220/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 749ms/step - loss: 1186.7809 - mae: 1187.2809 - val_loss: 29.5760 - val_mae: 30.0000\n",
      "Epoch 221/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 1186.7799 - mae: 1187.2799 - val_loss: 29.5759 - val_mae: 30.0000\n",
      "Epoch 222/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - loss: 1186.7789 - mae: 1187.2789 - val_loss: 29.5757 - val_mae: 30.0000\n",
      "Epoch 223/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - loss: 1186.7780 - mae: 1187.2780 - val_loss: 29.5755 - val_mae: 30.0000\n",
      "Epoch 224/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - loss: 1186.7769 - mae: 1187.2769 - val_loss: 29.5753 - val_mae: 30.0000\n",
      "Epoch 225/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - loss: 1186.7759 - mae: 1187.2759 - val_loss: 29.5751 - val_mae: 30.0000\n",
      "Epoch 226/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - loss: 1186.7750 - mae: 1187.2750 - val_loss: 29.5749 - val_mae: 30.0000\n",
      "Epoch 227/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - loss: 1186.7739 - mae: 1187.2739 - val_loss: 29.5747 - val_mae: 30.0000\n",
      "Epoch 228/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 1186.7729 - mae: 1187.2729 - val_loss: 29.5745 - val_mae: 30.0000\n",
      "Epoch 229/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - loss: 1186.7720 - mae: 1187.2720 - val_loss: 29.5743 - val_mae: 30.0000\n",
      "Epoch 230/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 1186.7710 - mae: 1187.2710 - val_loss: 29.5741 - val_mae: 30.0000\n",
      "Epoch 231/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - loss: 1186.7700 - mae: 1187.2700 - val_loss: 29.5739 - val_mae: 30.0000\n",
      "Epoch 232/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 1186.7690 - mae: 1187.2690 - val_loss: 29.5737 - val_mae: 30.0000\n",
      "Epoch 233/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - loss: 1186.7681 - mae: 1187.2681 - val_loss: 29.5735 - val_mae: 30.0000\n",
      "Epoch 234/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - loss: 1186.7671 - mae: 1187.2671 - val_loss: 29.5733 - val_mae: 30.0000\n",
      "Epoch 235/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - loss: 1186.7660 - mae: 1187.2660 - val_loss: 29.5732 - val_mae: 30.0000\n",
      "Epoch 236/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - loss: 1186.7651 - mae: 1187.2651 - val_loss: 29.5730 - val_mae: 30.0000\n",
      "Epoch 237/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 1186.7640 - mae: 1187.2640 - val_loss: 29.5728 - val_mae: 30.0000\n",
      "Epoch 238/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - loss: 1186.7631 - mae: 1187.2631 - val_loss: 29.5726 - val_mae: 30.0000\n",
      "Epoch 239/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - loss: 1186.7621 - mae: 1187.2621 - val_loss: 29.5724 - val_mae: 30.0000\n",
      "Epoch 240/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - loss: 1186.7611 - mae: 1187.2611 - val_loss: 29.5722 - val_mae: 30.0000\n",
      "Epoch 241/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - loss: 1186.7599 - mae: 1187.2599 - val_loss: 29.5720 - val_mae: 30.0000\n",
      "Epoch 242/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 1186.7589 - mae: 1187.2589 - val_loss: 29.5718 - val_mae: 30.0000\n",
      "Epoch 243/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - loss: 1186.7579 - mae: 1187.2579 - val_loss: 29.5716 - val_mae: 30.0000\n",
      "Epoch 244/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 546ms/step - loss: 1186.7570 - mae: 1187.2570 - val_loss: 29.5714 - val_mae: 30.0000\n",
      "Epoch 245/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - loss: 1186.7559 - mae: 1187.2559 - val_loss: 29.5713 - val_mae: 30.0000\n",
      "Epoch 246/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - loss: 1186.7549 - mae: 1187.2549 - val_loss: 29.5711 - val_mae: 30.0000\n",
      "Epoch 247/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - loss: 1186.7539 - mae: 1187.2539 - val_loss: 29.5709 - val_mae: 30.0000\n",
      "Epoch 248/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - loss: 1186.7529 - mae: 1187.2529 - val_loss: 29.5707 - val_mae: 30.0000\n",
      "Epoch 249/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - loss: 1186.7520 - mae: 1187.2520 - val_loss: 29.5705 - val_mae: 30.0000\n",
      "Epoch 250/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - loss: 1186.7510 - mae: 1187.2510 - val_loss: 29.5703 - val_mae: 30.0000\n",
      "Epoch 251/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step - loss: 1186.7500 - mae: 1187.2500 - val_loss: 29.5701 - val_mae: 30.0000\n",
      "Epoch 252/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - loss: 1186.7490 - mae: 1187.2490 - val_loss: 29.5699 - val_mae: 30.0000\n",
      "Epoch 253/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584ms/step - loss: 1186.7480 - mae: 1187.2480 - val_loss: 29.5698 - val_mae: 30.0000\n",
      "Epoch 254/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - loss: 1186.7469 - mae: 1187.2469 - val_loss: 29.5696 - val_mae: 30.0000\n",
      "Epoch 255/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 1186.7461 - mae: 1187.2461 - val_loss: 29.5694 - val_mae: 30.0000\n",
      "Epoch 256/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 538ms/step - loss: 1186.7451 - mae: 1187.2451 - val_loss: 29.5692 - val_mae: 30.0000\n",
      "Epoch 257/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - loss: 1186.7441 - mae: 1187.2441 - val_loss: 29.5690 - val_mae: 30.0000\n",
      "Epoch 258/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - loss: 1186.7430 - mae: 1187.2430 - val_loss: 29.5688 - val_mae: 30.0000\n",
      "Epoch 259/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - loss: 1186.7421 - mae: 1187.2421 - val_loss: 29.5686 - val_mae: 30.0000\n",
      "Epoch 260/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - loss: 1186.7411 - mae: 1187.2411 - val_loss: 29.5685 - val_mae: 30.0000\n",
      "Epoch 261/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 1186.7401 - mae: 1187.2401 - val_loss: 29.5683 - val_mae: 30.0000\n",
      "Epoch 262/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411ms/step - loss: 1186.7389 - mae: 1187.2389 - val_loss: 29.5681 - val_mae: 30.0000\n",
      "Epoch 263/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 1186.7379 - mae: 1187.2379 - val_loss: 29.5679 - val_mae: 30.0000\n",
      "Epoch 264/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - loss: 1186.7369 - mae: 1187.2369 - val_loss: 29.5677 - val_mae: 30.0000\n",
      "Epoch 265/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - loss: 1186.7360 - mae: 1187.2360 - val_loss: 29.5675 - val_mae: 30.0000\n",
      "Epoch 266/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - loss: 1186.7349 - mae: 1187.2349 - val_loss: 29.5673 - val_mae: 30.0000\n",
      "Epoch 267/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366ms/step - loss: 1186.7339 - mae: 1187.2339 - val_loss: 29.5672 - val_mae: 30.0000\n",
      "Epoch 268/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - loss: 1186.7329 - mae: 1187.2329 - val_loss: 29.5670 - val_mae: 30.0000\n",
      "Epoch 269/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - loss: 1186.7319 - mae: 1187.2319 - val_loss: 29.5668 - val_mae: 30.0000\n",
      "Epoch 270/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - loss: 1186.7310 - mae: 1187.2310 - val_loss: 29.5666 - val_mae: 30.0000\n",
      "Epoch 271/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - loss: 1186.7300 - mae: 1187.2300 - val_loss: 29.5664 - val_mae: 30.0000\n",
      "Epoch 272/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - loss: 1186.7290 - mae: 1187.2290 - val_loss: 29.5662 - val_mae: 30.0000\n",
      "Epoch 273/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - loss: 1186.7280 - mae: 1187.2280 - val_loss: 29.5661 - val_mae: 30.0000\n",
      "Epoch 274/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - loss: 1186.7271 - mae: 1187.2271 - val_loss: 29.5659 - val_mae: 30.0000\n",
      "Epoch 275/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - loss: 1186.7260 - mae: 1187.2260 - val_loss: 29.5657 - val_mae: 30.0000\n",
      "Epoch 276/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - loss: 1186.7250 - mae: 1187.2250 - val_loss: 29.5655 - val_mae: 30.0000\n",
      "Epoch 277/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step - loss: 1186.7241 - mae: 1187.2241 - val_loss: 29.5653 - val_mae: 30.0000\n",
      "Epoch 278/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - loss: 1186.7231 - mae: 1187.2231 - val_loss: 29.5652 - val_mae: 30.0000\n",
      "Epoch 279/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - loss: 1186.7220 - mae: 1187.2220 - val_loss: 29.5650 - val_mae: 30.0000\n",
      "Epoch 280/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step - loss: 1186.7211 - mae: 1187.2211 - val_loss: 29.5648 - val_mae: 30.0000\n",
      "Epoch 281/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step - loss: 1186.7201 - mae: 1187.2201 - val_loss: 29.5646 - val_mae: 30.0000\n",
      "Epoch 282/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - loss: 1186.7191 - mae: 1187.2191 - val_loss: 29.5644 - val_mae: 30.0000\n",
      "Epoch 283/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - loss: 1186.7179 - mae: 1187.2179 - val_loss: 29.5643 - val_mae: 30.0000\n",
      "Epoch 284/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - loss: 1186.7169 - mae: 1187.2169 - val_loss: 29.5641 - val_mae: 30.0000\n",
      "Epoch 285/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - loss: 1186.7159 - mae: 1187.2159 - val_loss: 29.5639 - val_mae: 30.0000\n",
      "Epoch 286/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 1186.7150 - mae: 1187.2150 - val_loss: 29.5637 - val_mae: 30.0000\n",
      "Epoch 287/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - loss: 1186.7139 - mae: 1187.2139 - val_loss: 29.5635 - val_mae: 30.0000\n",
      "Epoch 288/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - loss: 1186.7129 - mae: 1187.2129 - val_loss: 29.5634 - val_mae: 30.0000\n",
      "Epoch 289/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step - loss: 1186.7119 - mae: 1187.2119 - val_loss: 29.5632 - val_mae: 30.0000\n",
      "Epoch 290/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 1186.7109 - mae: 1187.2109 - val_loss: 29.5630 - val_mae: 30.0000\n",
      "Epoch 291/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step - loss: 1186.7100 - mae: 1187.2100 - val_loss: 29.5628 - val_mae: 30.0000\n",
      "Epoch 292/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - loss: 1186.7090 - mae: 1187.2090 - val_loss: 29.5627 - val_mae: 30.0000\n",
      "Epoch 293/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628ms/step - loss: 1186.7080 - mae: 1187.2080 - val_loss: 29.5625 - val_mae: 30.0000\n",
      "Epoch 294/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - loss: 1186.7070 - mae: 1187.2070 - val_loss: 29.5623 - val_mae: 30.0000\n",
      "Epoch 295/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - loss: 1186.7061 - mae: 1187.2061 - val_loss: 29.5621 - val_mae: 30.0000\n",
      "Epoch 296/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - loss: 1186.7051 - mae: 1187.2051 - val_loss: 29.5620 - val_mae: 30.0000\n",
      "Epoch 297/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - loss: 1186.7041 - mae: 1187.2041 - val_loss: 29.5618 - val_mae: 30.0000\n",
      "Epoch 298/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - loss: 1186.7031 - mae: 1187.2031 - val_loss: 29.5616 - val_mae: 30.0000\n",
      "Epoch 299/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 1186.7020 - mae: 1187.2020 - val_loss: 29.5614 - val_mae: 30.0000\n",
      "Epoch 300/300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419ms/step - loss: 1186.7010 - mae: 1187.2010 - val_loss: 29.5613 - val_mae: 30.0000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 1207.6224 - mae: 1208.0000\n",
      "Test Loss: 1207.6224365234375, Test MAE: 1208.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273ms/step\n",
      "Predictions saved to 'predicted_2024_2033.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "# Load datasets (replace file paths with actual locations of your datasets)\n",
    "data_2019_2023 = pd.read_excel(\"top 5+ stock 2019-2023.xlsx\")\n",
    "data_2024_2033 = pd.read_excel(\"top 5 2024_2033.xlsx\")\n",
    "\n",
    "# Extract relevant data\n",
    "repurchase_data = data_2019_2023[data_2019_2023['Parameters'] == 'repurchase of common stock']\n",
    "top_5_parameters = data_2019_2023[data_2019_2023['Parameters'] != 'repurchase of common stock']\n",
    "\n",
    "# Reshape and align datasets\n",
    "# Melt datasets to align by year\n",
    "top_5_data_flattened = top_5_parameters.melt(id_vars=['Company_name', 'Parameters'], \n",
    "                                             var_name='Year', \n",
    "                                             value_name='Value')\n",
    "repurchase_flattened = repurchase_data.melt(id_vars=['Company_name', 'Parameters'], \n",
    "                                            var_name='Year', \n",
    "                                            value_name='Repurchase')\n",
    "\n",
    "# Merge datasets\n",
    "merged_data = pd.merge(top_5_data_flattened, repurchase_flattened, on=['Company_name', 'Year'])\n",
    "merged_data.rename(columns={'Parameters_x': 'Parameters', 'Parameters_y': 'Repurchase_Parameter'}, inplace=True)\n",
    "\n",
    "# Compute top 5 parameters by correlation\n",
    "correlations = (\n",
    "    merged_data.groupby('Parameters')\n",
    "    .apply(lambda group: group['Value'].corr(group['Repurchase']))\n",
    "    .sort_values(ascending=False)\n",
    "    .head(5)\n",
    ")\n",
    "top_5_selected_params = correlations.index.tolist()\n",
    "\n",
    "# Filter data for top 5 parameters\n",
    "filtered_data = merged_data[merged_data['Parameters'].isin(top_5_selected_params)]\n",
    "\n",
    "# Prepare features and target\n",
    "data_pivoted = filtered_data.pivot_table(index=['Company_name', 'Year'], \n",
    "                                         columns='Parameters', \n",
    "                                         values='Value')\n",
    "target = filtered_data.drop_duplicates(subset=['Company_name', 'Year']).set_index(['Company_name', 'Year'])['Repurchase']\n",
    "\n",
    "# Align features and target\n",
    "data_pivoted, target = data_pivoted.align(target, join='inner', axis=0)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_pivoted, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build an enhanced neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    BatchNormalization(),  # Normalize activations\n",
    "    Dropout(0.3),  # Add dropout for regularization\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model with Huber loss\n",
    "model.compile(optimizer='adam', loss=Huber(), metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=300, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")\n",
    "\n",
    "# Load forecast data for 2024-2033\n",
    "forecast_data_flattened = data_2024_2033.melt(id_vars=['Company_name', 'Parameters'], \n",
    "                                              var_name='Year', \n",
    "                                              value_name='Value')\n",
    "forecast_filtered = forecast_data_flattened[forecast_data_flattened['Parameters'].isin(top_5_selected_params)]\n",
    "\n",
    "# Prepare features for prediction\n",
    "forecast_features = forecast_filtered.pivot_table(index=['Company_name', 'Year'], \n",
    "                                                   columns='Parameters', \n",
    "                                                   values='Value')\n",
    "\n",
    "# Standardize forecast features\n",
    "forecast_features_scaled = scaler.transform(forecast_features)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(forecast_features_scaled)\n",
    "\n",
    "# Prepare output\n",
    "forecast_features['Predicted_Repurchase'] = predictions\n",
    "forecast_features.reset_index(inplace=True)\n",
    "forecast_features = forecast_features[['Company_name', 'Year', 'Predicted_Repurchase']]\n",
    "\n",
    "# Save predictions to CSV\n",
    "forecast_features.to_csv(\"predicted_repurchase_2024_2033.csv\", index=False)\n",
    "print(\"Predictions saved to 'predicted_2024_2033.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
