{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred: 'Year'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and prepare the datasets\n",
    "def load_data(financial_data_path, stock_data_path):\n",
    "    # Load the financial metrics data\n",
    "    financial_df = pd.read_csv(financial_data_path)\n",
    "    \n",
    "    # Load the historical stock price data\n",
    "    stock_df = pd.read_csv(stock_data_path)\n",
    "    \n",
    "    return financial_df, stock_df\n",
    "\n",
    "def preprocess_data(financial_df, stock_df):\n",
    "    # Reshape financial data from wide to long format for easier processing\n",
    "    financial_long = pd.melt(\n",
    "        financial_df, \n",
    "        id_vars=['Company_name', 'Parameters'], \n",
    "        var_name='Year', \n",
    "        value_name='Value'\n",
    "    )\n",
    "    financial_long['Year'] = financial_long['Year'].astype(int)\n",
    "    \n",
    "    # Create a pivot table to have parameters as columns\n",
    "    financial_pivot = financial_long.pivot_table(\n",
    "        index=['Company_name', 'Year'], \n",
    "        columns='Parameters', \n",
    "        values='Value'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Ensure all column names are strings\n",
    "    financial_pivot.columns = [str(col) for col in financial_pivot.columns]\n",
    "    \n",
    "    # Rename stock repurchase column to avoid confusion\n",
    "    stock_df = stock_df.rename(columns={'repurchase of common stock': 'historical_stock_repurchase'})\n",
    "    \n",
    "    return financial_pivot, stock_df\n",
    "\n",
    "def train_models(financial_pivot, stock_df):\n",
    "    # Dictionary to store models for each company\n",
    "    company_models = {}\n",
    "    company_scalers_X = {}\n",
    "    company_scalers_y = {}\n",
    "    \n",
    "    # List of companies\n",
    "    companies = financial_pivot['Company_name'].unique()\n",
    "    \n",
    "    for company in companies:\n",
    "        # Filter data for this company\n",
    "        company_financial = financial_pivot[financial_pivot['Company_name'] == company]\n",
    "        company_stock = stock_df[stock_df['Company_name'] == company]\n",
    "        \n",
    "        # Merge the datasets on company name and year for historical period (2019-2023)\n",
    "        merged_df = pd.merge(\n",
    "            company_financial, \n",
    "            company_stock,\n",
    "            on=['Company_name', 'Year'], \n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        if len(merged_df) >= 3:  # Need at least 3 data points for meaningful regression\n",
    "            # Extract features (X) and target (y)\n",
    "            X = merged_df.drop(['Company_name', 'Year', 'historical_stock_repurchase'], axis=1)\n",
    "            y = merged_df['historical_stock_repurchase']\n",
    "            \n",
    "            # Standardize the data\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "            \n",
    "            X_scaled = scaler_X.fit_transform(X)\n",
    "            y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Train a linear regression model\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_scaled, y_scaled)\n",
    "            \n",
    "            # Store the model and scalers\n",
    "            company_models[company] = model\n",
    "            company_scalers_X[company] = scaler_X\n",
    "            company_scalers_y[company] = scaler_y\n",
    "    \n",
    "    return company_models, company_scalers_X, company_scalers_y\n",
    "\n",
    "def predict_stock_prices(financial_pivot, company_models, company_scalers_X, company_scalers_y, start_year, end_year):\n",
    "    # Dictionary to store predictions\n",
    "    predictions = {}\n",
    "    \n",
    "    # List of companies\n",
    "    companies = financial_pivot['Company_name'].unique()\n",
    "    \n",
    "    for company in companies:\n",
    "        if company not in company_models:\n",
    "            continue\n",
    "            \n",
    "        # Filter data for this company and the specified years\n",
    "        company_data = financial_pivot[\n",
    "            (financial_pivot['Company_name'] == company) & \n",
    "            (financial_pivot['Year'] >= start_year) & \n",
    "            (financial_pivot['Year'] <= end_year)\n",
    "        ]\n",
    "        \n",
    "        if len(company_data) > 0:\n",
    "            # Extract features\n",
    "            X_pred = company_data.drop(['Company_name', 'Year'], axis=1)\n",
    "            \n",
    "            # Standardize using the same scaler used for training\n",
    "            X_pred_scaled = company_scalers_X[company].transform(X_pred)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred_scaled = company_models[company].predict(X_pred_scaled)\n",
    "            \n",
    "            # Inverse transform to get the actual stock price predictions\n",
    "            y_pred = company_scalers_y[company].inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Store predictions along with years\n",
    "            years = company_data['Year'].values\n",
    "            predictions[company] = {year: pred for year, pred in zip(years, y_pred)}\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def format_predictions(predictions, start_year, end_year):\n",
    "    # Create a list to store formatted results\n",
    "    results = []\n",
    "    \n",
    "    # Years range\n",
    "    years = list(range(start_year, end_year + 1))\n",
    "    \n",
    "    # Add data for each company\n",
    "    for company, company_predictions in predictions.items():\n",
    "        row = {'Company_name': company}\n",
    "        for year in years:\n",
    "            if year in company_predictions:\n",
    "                row[str(year)] = round(company_predictions[year], 2)\n",
    "            else:\n",
    "                row[str(year)] = None\n",
    "        results.append(row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def plot_predictions(predictions, start_year, end_year):\n",
    "    # Create a figure for plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Years range\n",
    "    years = list(range(start_year, end_year + 1))\n",
    "    \n",
    "    # Plot predictions for each company\n",
    "    for company, company_predictions in predictions.items():\n",
    "        company_years = [year for year in years if year in company_predictions]\n",
    "        company_values = [company_predictions[year] for year in company_years]\n",
    "        plt.plot(company_years, company_values, marker='o', label=company)\n",
    "    \n",
    "    plt.title('Stock Price Predictions')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def predict_stock_for_timeframe(financial_data_path, stock_data_path, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Main function to predict stock prices for the specified timeframe\n",
    "    \n",
    "    Parameters:\n",
    "    financial_data_path (str): Path to the financial metrics CSV file\n",
    "    stock_data_path (str): Path to the historical stock price CSV file\n",
    "    start_year (int): Start year for predictions (should be >= 2024)\n",
    "    end_year (int): End year for predictions\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Predicted stock prices for each company and year\n",
    "    \"\"\"\n",
    "    # Validate input years\n",
    "    if start_year < 2024:\n",
    "        raise ValueError(\"Start year must be 2024 or later as 2023 is already in historical data\")\n",
    "    if end_year < start_year:\n",
    "        raise ValueError(\"End year must be greater than or equal to start year\")\n",
    "    if end_year > 2033:\n",
    "        raise ValueError(\"End year cannot exceed 2033 based on available future financial projections\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    financial_df, stock_df = load_data(financial_data_path, stock_data_path)\n",
    "    financial_pivot, stock_df = preprocess_data(financial_df, stock_df)\n",
    "    \n",
    "    # Train models\n",
    "    company_models, company_scalers_X, company_scalers_y = train_models(financial_pivot, stock_df)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = predict_stock_prices(\n",
    "        financial_pivot, company_models, company_scalers_X, company_scalers_y, start_year, end_year\n",
    "    )\n",
    "    \n",
    "    # Format predictions\n",
    "    results_df = format_predictions(predictions, start_year, end_year)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to your CSV files\n",
    "    financial_data_path = \"merged.csv\"\n",
    "    stock_data_path = \"stock_historical.csv\"\n",
    "    \n",
    "    # Define start and end years for prediction\n",
    "    start_year = 2024\n",
    "    end_year = 2030\n",
    "    \n",
    "    # Get predictions\n",
    "    try:\n",
    "        predictions_df = predict_stock_for_timeframe(\n",
    "            financial_data_path, stock_data_path, start_year, end_year\n",
    "        )\n",
    "        \n",
    "        # Display the predictions\n",
    "        print(\"\\nStock Price Predictions:\")\n",
    "        print(predictions_df)\n",
    "        \n",
    "        # You can also save to CSV\n",
    "        predictions_df.to_csv(\"stock_predictions.csv\", index=False)\n",
    "        print(\"\\nPredictions saved to 'stock_predictions.csv'\")\n",
    "        \n",
    "        # Optional: Load and preprocess data for visualization\n",
    "        financial_df, stock_df = load_data(financial_data_path, stock_data_path)\n",
    "        financial_pivot, stock_df = preprocess_data(financial_df, stock_df)\n",
    "        \n",
    "        # Train models\n",
    "        company_models, company_scalers_X, company_scalers_y = train_models(financial_pivot, stock_df)\n",
    "        \n",
    "        # Get prediction data in dictionary format for plotting\n",
    "        predictions = predict_stock_prices(\n",
    "            financial_pivot, company_models, company_scalers_X, company_scalers_y, start_year, end_year\n",
    "        )\n",
    "        \n",
    "        # Plot predictions\n",
    "        plt = plot_predictions(predictions, start_year, end_year)\n",
    "        plt.savefig(\"stock_predictions.png\")\n",
    "        print(\"Prediction plot saved as 'stock_predictions.png'\")\n",
    "        plt.show()\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Variable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_220\\917614880.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[0mend_year\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2030\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m     \u001b[0mpredictions_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_stock_for_timeframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinancial_data_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstock_data_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_year\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_year\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_220\\917614880.py\u001b[0m in \u001b[0;36mpredict_stock_for_timeframe\u001b[1;34m(financial_data_path, stock_data_path, start_year, end_year)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[0mfinancial_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstock_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinancial_data_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstock_data_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m     \u001b[0mfinancial_pivot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstock_pivot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinancial_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstock_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[0mcompany_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompany_scalers_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompany_scalers_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinancial_pivot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstock_pivot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_220\\917614880.py\u001b[0m in \u001b[0;36mpreprocess_data\u001b[1;34m(financial_df, stock_df)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Pivot to get financial parameters as columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     financial_pivot = financial_long.pivot_table(\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Company_name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Year\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Variable\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mpivot_table\u001b[1;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m   9507\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpivot_table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9509\u001b[1;33m         return pivot_table(\n\u001b[0m\u001b[0;32m   9510\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9511\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pivot_table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     table = __internal_pivot_table(\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36m__internal_pivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[0mobserved_bool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mobserved\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_default\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mobserved\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0mgrouped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobserved\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobserved_bool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m     if observed is lib.no_default and any(\n\u001b[0;32m    174\u001b[0m         \u001b[0mping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_passed_categorical\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mping\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grouper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9181\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You have to supply one of 'by' and 'level'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9183\u001b[1;33m         return DataFrameGroupBy(\n\u001b[0m\u001b[0;32m   9184\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9185\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgrouper\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[0;32m   1330\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\groupby\\grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1041\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m             \u001b[1;31m# Add key to exclusions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Variable'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "def load_data(financial_data_path, stock_data_path):\n",
    "    financial_df = pd.read_csv(financial_data_path)\n",
    "    stock_df = pd.read_csv(stock_data_path)\n",
    "    return financial_df, stock_df\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(financial_df, stock_df):\n",
    "    # Reshape financial data\n",
    "    financial_long = pd.melt(\n",
    "        financial_df,\n",
    "        id_vars=[\"Company_name\"],\n",
    "        var_name=\"Year\",\n",
    "        value_name=\"Value\"\n",
    "    )\n",
    "\n",
    "    # Filter out rows where \"Year\" contains non-numeric values\n",
    "    financial_long = financial_long[financial_long[\"Year\"].str.isdigit()]\n",
    "\n",
    "    # Convert \"Year\" to integers\n",
    "    financial_long[\"Year\"] = financial_long[\"Year\"].astype(int)\n",
    "\n",
    "    # Pivot to get financial parameters as columns\n",
    "    financial_pivot = financial_long.pivot_table(\n",
    "        index=[\"Company_name\", \"Year\"],\n",
    "        columns=\"Variable\",\n",
    "        values=\"Value\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # Fill missing values\n",
    "    financial_pivot = financial_pivot.fillna(financial_pivot.mean(numeric_only=True))\n",
    "\n",
    "    # Process stock data\n",
    "    stock_pivot = stock_df.rename(columns={\"repurchase of common stock\": \"Stock_price\"})\n",
    "\n",
    "    return financial_pivot, stock_pivot\n",
    "\n",
    "\n",
    "# Train models for each company\n",
    "def train_models(financial_pivot, stock_pivot):\n",
    "    company_models = {}\n",
    "    company_scalers_X = {}\n",
    "    company_scalers_y = {}\n",
    "\n",
    "    companies = financial_pivot[\"Company_name\"].unique()\n",
    "\n",
    "    for company in companies:\n",
    "        company_financial = financial_pivot[financial_pivot[\"Company_name\"] == company]\n",
    "        company_stock = stock_pivot[stock_pivot[\"Company_name\"] == company]\n",
    "\n",
    "        merged_df = pd.merge(company_financial, company_stock, on=[\"Company_name\", \"Year\"], how=\"inner\")\n",
    "\n",
    "        if len(merged_df) >= 3:  # Need at least 3 data points for regression\n",
    "            X = merged_df.drop([\"Company_name\", \"Year\", \"Stock_price\"], axis=1)\n",
    "            y = merged_df[\"Stock_price\"]\n",
    "\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "\n",
    "            X_scaled = scaler_X.fit_transform(X)\n",
    "            y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_scaled, y_scaled)\n",
    "\n",
    "            company_models[company] = model\n",
    "            company_scalers_X[company] = scaler_X\n",
    "            company_scalers_y[company] = scaler_y\n",
    "\n",
    "    return company_models, company_scalers_X, company_scalers_y\n",
    "\n",
    "\n",
    "# Predict stock prices\n",
    "def predict_stock_prices(financial_pivot, company_models, company_scalers_X, company_scalers_y, start_year, end_year):\n",
    "    predictions = {}\n",
    "    companies = financial_pivot[\"Company_name\"].unique()\n",
    "\n",
    "    for company in companies:\n",
    "        if company not in company_models:\n",
    "            continue\n",
    "\n",
    "        company_data = financial_pivot[\n",
    "            (financial_pivot[\"Company_name\"] == company) &\n",
    "            (financial_pivot[\"Year\"] >= start_year) &\n",
    "            (financial_pivot[\"Year\"] <= end_year)\n",
    "        ]\n",
    "\n",
    "        if len(company_data) > 0:\n",
    "            X_pred = company_data.drop([\"Company_name\", \"Year\"], axis=1)\n",
    "            X_pred_scaled = company_scalers_X[company].transform(X_pred)\n",
    "\n",
    "            y_pred_scaled = company_models[company].predict(X_pred_scaled)\n",
    "            y_pred = company_scalers_y[company].inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "            years = company_data[\"Year\"].values\n",
    "            predictions[company] = {year: pred for year, pred in zip(years, y_pred)}\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Format predictions\n",
    "def format_predictions(predictions, start_year, end_year):\n",
    "    results = []\n",
    "    years = list(range(start_year, end_year + 1))\n",
    "\n",
    "    for company, company_predictions in predictions.items():\n",
    "        row = {\"Company_name\": company}\n",
    "        for year in years:\n",
    "            row[str(year)] = round(company_predictions.get(year, np.nan), 2)\n",
    "        results.append(row)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Plot predictions\n",
    "def plot_predictions(predictions, start_year, end_year):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    years = list(range(start_year, end_year + 1))\n",
    "\n",
    "    for company, company_predictions in predictions.items():\n",
    "        company_years = [year for year in years if year in company_predictions]\n",
    "        company_values = [company_predictions[year] for year in company_years]\n",
    "        plt.plot(company_years, company_values, marker=\"o\", label=company)\n",
    "\n",
    "    plt.title(\"Stock Price Predictions\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    return plt\n",
    "\n",
    "\n",
    "# Main function\n",
    "def predict_stock_for_timeframe(financial_data_path, stock_data_path, start_year, end_year):\n",
    "    if start_year < 2024:\n",
    "        raise ValueError(\"Start year must be 2024 or later as 2023 is already in historical data\")\n",
    "    if end_year < start_year:\n",
    "        raise ValueError(\"End year must be greater than or equal to start year\")\n",
    "    if end_year > 2033:\n",
    "        raise ValueError(\"End year cannot exceed 2033 based on available future financial projections\")\n",
    "\n",
    "    financial_df, stock_df = load_data(financial_data_path, stock_data_path)\n",
    "    financial_pivot, stock_pivot = preprocess_data(financial_df, stock_df)\n",
    "\n",
    "    company_models, company_scalers_X, company_scalers_y = train_models(financial_pivot, stock_pivot)\n",
    "\n",
    "    predictions = predict_stock_prices(\n",
    "        financial_pivot, company_models, company_scalers_X, company_scalers_y, start_year, end_year\n",
    "    )\n",
    "\n",
    "    results_df = format_predictions(predictions, start_year, end_year)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    financial_data_path = \"merged.csv\"\n",
    "    stock_data_path = \"stock_historical.csv\"\n",
    "    start_year = 2024\n",
    "    end_year = 2030\n",
    "\n",
    "    predictions_df = predict_stock_for_timeframe(financial_data_path, stock_data_path, start_year, end_year)\n",
    "    print(predictions_df)\n",
    "\n",
    "    predictions_df.to_csv(\"stock_predictions.csv\", index=False)\n",
    "    print(\"Predictions saved to 'stock_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_220\\494880372.py:30: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  filled_data = merged_data.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 7169328.0000 - mae: 1687.5111 - val_loss: 165731.4531 - val_mae: 371.5173\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - loss: 7168969.0000 - mae: 1687.5349 - val_loss: 165655.7500 - val_mae: 371.4158\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 7168947.5000 - mae: 1687.6205 - val_loss: 165579.5000 - val_mae: 371.3135\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 7167837.0000 - mae: 1687.4028 - val_loss: 165497.5000 - val_mae: 371.2029\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 7168062.0000 - mae: 1687.4957 - val_loss: 165420.0625 - val_mae: 371.0984\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 7167227.0000 - mae: 1687.2562 - val_loss: 165332.0781 - val_mae: 370.9788\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 7167914.0000 - mae: 1687.4695 - val_loss: 165249.3125 - val_mae: 370.8672\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597ms/step - loss: 7167157.5000 - mae: 1687.2859 - val_loss: 165163.5625 - val_mae: 370.7510\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 7166448.0000 - mae: 1687.1818 - val_loss: 165075.4531 - val_mae: 370.6320\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 7166520.5000 - mae: 1687.3121 - val_loss: 164985.4531 - val_mae: 370.5101\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 7166463.0000 - mae: 1687.3057 - val_loss: 164893.4688 - val_mae: 370.3855\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - loss: 7165506.0000 - mae: 1687.1506 - val_loss: 164794.2031 - val_mae: 370.2513\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - loss: 7165272.0000 - mae: 1686.9750 - val_loss: 164691.4531 - val_mae: 370.1125\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 7165745.0000 - mae: 1687.1805 - val_loss: 164588.4062 - val_mae: 369.9748\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - loss: 7165399.5000 - mae: 1687.1770 - val_loss: 164487.2969 - val_mae: 369.8387\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - loss: 7164767.5000 - mae: 1687.2078 - val_loss: 164386.0938 - val_mae: 369.7019\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 7165134.0000 - mae: 1687.2996 - val_loss: 164288.7500 - val_mae: 369.5699\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step - loss: 7164053.0000 - mae: 1687.1726 - val_loss: 164187.7344 - val_mae: 369.4324\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 7164954.0000 - mae: 1687.4208 - val_loss: 164089.2812 - val_mae: 369.2995\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - loss: 7163849.0000 - mae: 1687.1211 - val_loss: 163992.4688 - val_mae: 369.1685\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - loss: 7162580.0000 - mae: 1686.8068 - val_loss: 163892.5469 - val_mae: 369.0332\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 7162873.5000 - mae: 1686.9590 - val_loss: 163787.8281 - val_mae: 368.8913\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 7163165.5000 - mae: 1686.8523 - val_loss: 163680.9688 - val_mae: 368.7466\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - loss: 7162696.0000 - mae: 1686.9149 - val_loss: 163571.8438 - val_mae: 368.5989\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - loss: 7160843.0000 - mae: 1686.7158 - val_loss: 163460.8438 - val_mae: 368.4483\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 7161179.5000 - mae: 1686.8542 - val_loss: 163348.9844 - val_mae: 368.2963\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - loss: 7160385.5000 - mae: 1686.4963 - val_loss: 163229.3125 - val_mae: 368.1336\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - loss: 7159904.0000 - mae: 1686.5537 - val_loss: 163106.4531 - val_mae: 367.9668\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 7160101.0000 - mae: 1686.5255 - val_loss: 162980.4844 - val_mae: 367.7958\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 7160066.0000 - mae: 1686.7914 - val_loss: 162851.4688 - val_mae: 367.6205\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - loss: 7159174.5000 - mae: 1686.8865 - val_loss: 162725.3281 - val_mae: 367.4490\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - loss: 7158378.0000 - mae: 1686.7352 - val_loss: 162600.1562 - val_mae: 367.2795\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step - loss: 7157898.0000 - mae: 1686.6333 - val_loss: 162476.0312 - val_mae: 367.1113\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 7157713.5000 - mae: 1686.4698 - val_loss: 162351.9062 - val_mae: 366.9422\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - loss: 7156388.0000 - mae: 1686.3198 - val_loss: 162227.8906 - val_mae: 366.7732\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - loss: 7157405.5000 - mae: 1686.5375 - val_loss: 162098.4219 - val_mae: 366.5965\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 7156380.5000 - mae: 1686.3546 - val_loss: 161961.8906 - val_mae: 366.4106\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 7155315.0000 - mae: 1686.2710 - val_loss: 161822.0156 - val_mae: 366.2195\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 7153525.5000 - mae: 1685.8657 - val_loss: 161679.1250 - val_mae: 366.0245\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 7153759.5000 - mae: 1685.7700 - val_loss: 161531.2188 - val_mae: 365.8224\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 7151574.5000 - mae: 1685.7314 - val_loss: 161377.4375 - val_mae: 365.6131\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 7153608.0000 - mae: 1685.7841 - val_loss: 161211.4219 - val_mae: 365.3869\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - loss: 7149646.0000 - mae: 1685.2106 - val_loss: 161041.5781 - val_mae: 365.1550\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 7151069.0000 - mae: 1685.5181 - val_loss: 160866.2188 - val_mae: 364.9163\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 7150854.0000 - mae: 1685.5542 - val_loss: 160689.2656 - val_mae: 364.6744\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - loss: 7148428.5000 - mae: 1685.3491 - val_loss: 160503.9375 - val_mae: 364.4204\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - loss: 7147262.0000 - mae: 1685.0068 - val_loss: 160308.5312 - val_mae: 364.1523\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 7146706.5000 - mae: 1684.8441 - val_loss: 160110.6875 - val_mae: 363.8812\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 7148274.0000 - mae: 1685.8159 - val_loss: 159919.4219 - val_mae: 363.6188\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 7144021.5000 - mae: 1684.9135 - val_loss: 159726.4219 - val_mae: 363.3535\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Predicted stock price for 2024: -8.533169746398926\n",
      "Predicted stock price for 2025: -10.489684104919434\n",
      "Predicted stock price for 2026: -11.512456893920898\n",
      "Predicted stock price for 2027: -12.053183555603027\n",
      "Predicted stock price for 2028: -12.343695640563965\n",
      "Predicted stock price for 2029: -12.499776840209961\n",
      "Predicted stock price for 2030: -12.583612442016602\n",
      "Predicted stock price for 2031: -12.628500938415527\n",
      "Predicted stock price for 2032: -12.65253734588623\n",
      "Predicted stock price for 2033: -12.665406227111816\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Load the datasets\n",
    "parameters_df = pd.read_csv('merged.csv')  # Replace with actual path\n",
    "stock_historical_df = pd.read_csv('stock_historical.csv')  # Replace with actual path\n",
    "\n",
    "# Select the top 5 parameters for each company\n",
    "top_5_parameters = parameters_df.groupby('Company_name').head(5)\n",
    "\n",
    "# Pivot the data to have years as columns and parameters as rows\n",
    "top_5_pivot = top_5_parameters.pivot(index='Company_name', columns='Parameters')\n",
    "\n",
    "# Flatten multi-level columns in top_5_pivot\n",
    "top_5_pivot.columns = ['_'.join(map(str, col)) if isinstance(col, tuple) else col for col in top_5_pivot.columns]\n",
    "top_5_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Merge the parameters dataset with historical stock data\n",
    "merged_data = pd.merge(\n",
    "    stock_historical_df,\n",
    "    top_5_pivot,\n",
    "    on='Company_name',\n",
    "    suffixes=('_stock', '_params')\n",
    ")\n",
    "\n",
    "# Handle missing values by forward-filling and backward-filling\n",
    "filled_data = merged_data.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "# Extract input features (parameters and historical stock prices) and outputs\n",
    "input_columns = [\n",
    "    col for col in filled_data.columns if (\"2019\" <= col <= \"2023\" or \"_2019\" <= col <= \"_2023\")\n",
    "]\n",
    "X = filled_data[input_columns].values\n",
    "y = filled_data[\"2023\"].values  # Use 2023 as a sample supervised learning target\n",
    "\n",
    "# Normalize the input data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear')  # Output layer for stock price prediction\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=4, verbose=1)\n",
    "\n",
    "# Predict stock prices for future years (e.g., 2024 to 2033)\n",
    "future_years = [2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033]\n",
    "future_predictions = []\n",
    "\n",
    "# Use the last available data as a starting point for future predictions\n",
    "last_data = X_scaled[-1].reshape(1, -1)\n",
    "\n",
    "for year in future_years:\n",
    "    prediction = model.predict(last_data)\n",
    "    future_predictions.append(prediction[0][0])\n",
    "    # Update the input for the next prediction (simulating new parameter values if needed)\n",
    "    last_data[0][-1] = prediction[0][0]  # Replace the last stock price with the predicted value\n",
    "\n",
    "# Display the future predictions\n",
    "for year, pred in zip(future_years, future_predictions):\n",
    "    print(f\"Predicted stock price for {year}: {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping BKR due to insufficient data (n_samples=1).\n",
      "Skipping FTI due to insufficient data (n_samples=1).\n",
      "Skipping HAL due to insufficient data (n_samples=1).\n",
      "Skipping NOV due to insufficient data (n_samples=1).\n",
      "Skipping SLB due to insufficient data (n_samples=1).\n",
      "Skipping WHD due to insufficient data (n_samples=1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_220\\3210481136.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  filled_data = merged_data.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "parameters_df = pd.read_csv('merged.csv')  # Replace with actual path\n",
    "stock_historical_df = pd.read_csv('stock_historical.csv')  # Replace with actual path\n",
    "\n",
    "# Select the top 5 parameters for each company\n",
    "top_5_parameters = parameters_df.groupby('Company_name').head(5)\n",
    "\n",
    "# Pivot the data to have years as columns and parameters as rows\n",
    "top_5_pivot = top_5_parameters.pivot(index='Company_name', columns='Parameters')\n",
    "\n",
    "# Flatten multi-level columns in top_5_pivot\n",
    "top_5_pivot.columns = ['_'.join(map(str, col)) if isinstance(col, tuple) else col for col in top_5_pivot.columns]\n",
    "top_5_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Merge the parameters dataset with historical stock data\n",
    "merged_data = pd.merge(\n",
    "    stock_historical_df,\n",
    "    top_5_pivot,\n",
    "    on='Company_name',\n",
    "    suffixes=('_stock', '_params')\n",
    ")\n",
    "\n",
    "# Handle missing values by forward-filling and backward-filling\n",
    "filled_data = merged_data.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "# Group by company to train and predict separately for each company\n",
    "companies = filled_data['Company_name'].unique()\n",
    "\n",
    "# Dictionary to store predictions for each company\n",
    "predictions_per_company = {}\n",
    "\n",
    "for company in companies:\n",
    "    # Filter data for the current company\n",
    "    company_data = filled_data[filled_data['Company_name'] == company]\n",
    "\n",
    "    # Extract input features and target output\n",
    "    input_columns = [\n",
    "        col for col in company_data.columns if (\"2019\" <= col <= \"2023\" or \"_2019\" <= col <= \"_2023\")\n",
    "    ]\n",
    "    X = company_data[input_columns].values\n",
    "    y = company_data[\"2023\"].values  # Use 2023 as a supervised learning target\n",
    "\n",
    "    # Check if the company has sufficient data\n",
    "    if len(X) < 2:\n",
    "        print(f\"Skipping {company} due to insufficient data (n_samples={len(X)}).\")\n",
    "        continue\n",
    "\n",
    "    # Normalize the input data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the neural network model\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='linear')  # Output layer for stock price prediction\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=4, verbose=0)\n",
    "\n",
    "    # Predict stock prices for future years (e.g., 2024 to 2033)\n",
    "    future_years = [2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033]\n",
    "    future_predictions = []\n",
    "\n",
    "    # Use the last available data as a starting point for future predictions\n",
    "    last_data = X_scaled[-1].reshape(1, -1)\n",
    "\n",
    "    for year in future_years:\n",
    "        prediction = model.predict(last_data)\n",
    "        future_predictions.append(prediction[0][0])\n",
    "        # Update the input for the next prediction (simulating new parameter values if needed)\n",
    "        last_data[0][-1] = prediction[0][0]  # Replace the last stock price with the predicted value\n",
    "\n",
    "    # Store predictions for the current company\n",
    "    predictions_per_company[company] = {year: pred for year, pred in zip(future_years, future_predictions)}\n",
    "\n",
    "# Display the predictions for each company\n",
    "for company, predictions in predictions_per_company.items():\n",
    "    print(f\"Predictions for {company}:\")\n",
    "    for year, pred in predictions.items():\n",
    "        print(f\"  Year {year}: {pred:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
