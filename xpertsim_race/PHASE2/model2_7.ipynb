{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data head:\n",
      "  Company_name                                   Parameters    2019    2020  \\\n",
      "0          BKR                   repurchase of common stock     0.0     0.0   \n",
      "1          BKR                               dividends paid  -630.0  -278.0   \n",
      "2          BKR  net cash flows used in financing activities  -695.0 -1007.0   \n",
      "3          BKR  net cash flows used in investing activities -1445.0  -486.0   \n",
      "4          BKR     proceeds from sales of equity securities  2669.0  2666.0   \n",
      "\n",
      "     2021    2022    2023  \n",
      "0  -434.0  -828.0  -538.0  \n",
      "1  -161.0  -435.0  -576.0  \n",
      "2  -838.0 -1798.0 -1671.0  \n",
      "3  -534.0  -967.0 -1659.0  \n",
      "4  2665.0  2664.0  2663.0  \n",
      "\n",
      "Forecast data head:\n",
      "  Company_name                                   Parameters      2024  \\\n",
      "0          BKR                               dividends paid  -570.824   \n",
      "1          BKR  net cash flows used in financing activities -1917.007   \n",
      "2          BKR  net cash flows used in investing activities -1714.013   \n",
      "3          BKR     proceeds from sales of equity securities  2661.496   \n",
      "4          BKR                         total current assets -1381.482   \n",
      "\n",
      "       2025      2026      2027       2028       2029       2030       2031  \\\n",
      "0  -534.631  -509.237  -497.090   -486.353   -490.984   -487.396   -463.032   \n",
      "1 -2133.827 -2367.924 -2611.155  -2870.269  -3100.475  -3349.806  -3584.737   \n",
      "2 -1792.028 -1824.691 -1899.551  -1928.034  -1989.011  -2040.778  -2145.299   \n",
      "3  2660.008  2658.518  2657.004   2655.510   2654.016   2652.564   2651.018   \n",
      "4 -3988.433 -6481.101 -8910.690 -11031.527 -13706.057 -16286.657 -19242.320   \n",
      "\n",
      "        2032       2033  \n",
      "0   -451.536   -438.121  \n",
      "1  -3823.404  -4060.690  \n",
      "2  -2261.899  -2280.911  \n",
      "3   2649.566   2648.072  \n",
      "4 -21227.288 -23950.316  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 518ms/step - loss: 1604371.3750 - mae: 535.7933 - val_loss: 5266856.0000 - val_mae: 1330.5236\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 1659614.6250 - mae: 579.1215 - val_loss: 5266442.0000 - val_mae: 1330.4481\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 1610938.6250 - mae: 549.4907 - val_loss: 5266068.0000 - val_mae: 1330.3800\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - loss: 1632803.7500 - mae: 573.8770 - val_loss: 5265672.0000 - val_mae: 1330.3092\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 1649783.3750 - mae: 567.7875 - val_loss: 5265272.5000 - val_mae: 1330.2366\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - loss: 1616679.7500 - mae: 550.2119 - val_loss: 5264880.5000 - val_mae: 1330.1641\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - loss: 994341.6875 - mae: 413.1843 - val_loss: 5264519.0000 - val_mae: 1330.0948\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - loss: 1609492.0000 - mae: 550.1619 - val_loss: 5264130.0000 - val_mae: 1330.0219\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 1656132.1250 - mae: 569.4309 - val_loss: 5263738.0000 - val_mae: 1329.9484\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 1617533.7500 - mae: 553.9856 - val_loss: 5263350.5000 - val_mae: 1329.8759\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 1035410.2500 - mae: 435.9913 - val_loss: 5262960.5000 - val_mae: 1329.8021\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1641794.8750 - mae: 559.4977 - val_loss: 5262539.5000 - val_mae: 1329.7209\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - loss: 1562882.2500 - mae: 516.7719 - val_loss: 5262118.5000 - val_mae: 1329.6373\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 1083299.3750 - mae: 470.4986 - val_loss: 5261678.5000 - val_mae: 1329.5504\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 1611296.0000 - mae: 546.4428 - val_loss: 5261207.0000 - val_mae: 1329.4598\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 1646505.7500 - mae: 565.7090 - val_loss: 5260727.5000 - val_mae: 1329.3682\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - loss: 1650997.7500 - mae: 570.1357 - val_loss: 5260248.5000 - val_mae: 1329.2738\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 1592496.5000 - mae: 534.1534 - val_loss: 5259761.0000 - val_mae: 1329.1779\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - loss: 1566807.7500 - mae: 522.6671 - val_loss: 5259248.0000 - val_mae: 1329.0778\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232ms/step - loss: 1645836.8750 - mae: 563.8698 - val_loss: 5258706.5000 - val_mae: 1328.9725\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 1607342.8750 - mae: 537.8194 - val_loss: 5258157.5000 - val_mae: 1328.8656\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 1031142.5000 - mae: 434.7381 - val_loss: 5257587.5000 - val_mae: 1328.7550\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 1571604.1250 - mae: 532.6641 - val_loss: 5256949.5000 - val_mae: 1328.6345\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 986784.0625 - mae: 405.9285 - val_loss: 5256301.5000 - val_mae: 1328.5121\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 1097262.2500 - mae: 482.8242 - val_loss: 5255585.5000 - val_mae: 1328.3800\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 1663004.7500 - mae: 589.7802 - val_loss: 5254829.5000 - val_mae: 1328.2429\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - loss: 1039344.0000 - mae: 441.2180 - val_loss: 5254079.5000 - val_mae: 1328.1075\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 174ms/step - loss: 1644902.7500 - mae: 563.9246 - val_loss: 5253238.5000 - val_mae: 1327.9600\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 1642656.7500 - mae: 564.1639 - val_loss: 5252382.0000 - val_mae: 1327.8091\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 1653679.5000 - mae: 575.9047 - val_loss: 5251516.5000 - val_mae: 1327.6566\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 1613810.8750 - mae: 555.9915 - val_loss: 5250626.5000 - val_mae: 1327.5001\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 1668043.0000 - mae: 597.5778 - val_loss: 5249700.0000 - val_mae: 1327.3381\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 1620588.3750 - mae: 570.1900 - val_loss: 5248752.5000 - val_mae: 1327.1733\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398ms/step - loss: 1644266.1250 - mae: 568.0187 - val_loss: 5247766.5000 - val_mae: 1327.0024\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268ms/step - loss: 1070317.5000 - mae: 455.3622 - val_loss: 5246756.5000 - val_mae: 1326.8275\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - loss: 1559213.2500 - mae: 515.3521 - val_loss: 5245622.5000 - val_mae: 1326.6354\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - loss: 1019134.8125 - mae: 424.8508 - val_loss: 5244434.5000 - val_mae: 1326.4349\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step - loss: 1594900.8750 - mae: 538.7532 - val_loss: 5243119.0000 - val_mae: 1326.2137\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 1044788.6250 - mae: 448.8791 - val_loss: 5241779.5000 - val_mae: 1325.9882\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - loss: 1614548.0000 - mae: 552.0068 - val_loss: 5240312.5000 - val_mae: 1325.7465\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 1605065.7500 - mae: 552.0235 - val_loss: 5238818.5000 - val_mae: 1325.5005\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - loss: 1588850.1250 - mae: 529.1546 - val_loss: 5237295.5000 - val_mae: 1325.2495\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 1089372.8750 - mae: 484.2302 - val_loss: 5235732.0000 - val_mae: 1324.9918\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 1042033.8750 - mae: 453.3433 - val_loss: 5234052.5000 - val_mae: 1324.7205\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 1602087.6250 - mae: 555.5087 - val_loss: 5232230.5000 - val_mae: 1324.4296\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1658467.3750 - mae: 591.8192 - val_loss: 5230397.0000 - val_mae: 1324.1368\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 1651902.6250 - mae: 581.2297 - val_loss: 5228586.0000 - val_mae: 1323.8474\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 1638593.7500 - mae: 567.8717 - val_loss: 5226788.5000 - val_mae: 1323.5587\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 1024888.9375 - mae: 433.9640 - val_loss: 5224987.5000 - val_mae: 1323.2679\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 1657099.6250 - mae: 589.2266 - val_loss: 5222988.5000 - val_mae: 1322.9501\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No matching data found in forecast data for 'Repurchase of Common Stock'. Check the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7028\\1545613032.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0mforecast_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforecast_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mforecast_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Parameters'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'repurchase of common stock'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mforecast_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No matching data found in forecast data for 'Repurchase of Common Stock'. Check the dataset.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m forecast_data = forecast_data.melt(id_vars=['Company_name', 'Parameters'], \n",
      "\u001b[1;31mValueError\u001b[0m: No matching data found in forecast data for 'Repurchase of Common Stock'. Check the dataset."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load datasets\n",
    "train_data_path = 'top 5+ stock 2019-2023.xlsx'\n",
    "forecast_data_path = 'top 5 2024_2033.xlsx'\n",
    "\n",
    "# Read the data\n",
    "train_data = pd.ExcelFile(train_data_path).parse(0)  # Assuming the first sheet contains data\n",
    "forecast_data = pd.ExcelFile(forecast_data_path).parse(0)  # Assuming the first sheet contains data\n",
    "\n",
    "# Inspect the data\n",
    "print(\"Training data head:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nForecast data head:\")\n",
    "print(forecast_data.head())\n",
    "\n",
    "# Filter and reshape the training data\n",
    "train_data = train_data[train_data['Parameters'].str.strip().str.lower() == 'repurchase of common stock']\n",
    "if train_data.empty:\n",
    "    raise ValueError(\"No matching data found in training data for 'Repurchase of Common Stock'. Check the dataset.\")\n",
    "\n",
    "train_data = train_data.melt(id_vars=['Company_name', 'Parameters'], \n",
    "                             var_name='Year', \n",
    "                             value_name='Repurchase')\n",
    "train_data['Year'] = train_data['Year'].astype(int)\n",
    "\n",
    "# Select features and target\n",
    "X = train_data[['Company_name', 'Year']]  # Features\n",
    "y = train_data['Repurchase']  # Target variable\n",
    "\n",
    "# Encode categorical variables (if any)\n",
    "X = pd.get_dummies(X, columns=['Company_name'], drop_first=True)\n",
    "\n",
    "# Check if X is empty after encoding\n",
    "if X.empty:\n",
    "    raise ValueError(\"Encoded feature set is empty. Check categorical encoding.\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "# Preprocess the forecast data\n",
    "forecast_data = forecast_data[forecast_data['Parameters'].str.strip().str.lower() == 'repurchase of common stock']\n",
    "if forecast_data.empty:\n",
    "    raise ValueError(\"No matching data found in forecast data for 'Repurchase of Common Stock'. Check the dataset.\")\n",
    "\n",
    "forecast_data = forecast_data.melt(id_vars=['Company_name', 'Parameters'], \n",
    "                                   var_name='Year', \n",
    "                                   value_name='Repurchase')\n",
    "forecast_data['Year'] = forecast_data['Year'].astype(int)\n",
    "\n",
    "forecast_X = forecast_data[['Company_name', 'Year']]\n",
    "forecast_X = pd.get_dummies(forecast_X, columns=['Company_name'], drop_first=True)\n",
    "forecast_X_scaled = scaler.transform(forecast_X)\n",
    "\n",
    "# Predict repurchase values for 2024-2033\n",
    "forecast_data['Repurchase_Predicted'] = model.predict(forecast_X_scaled)\n",
    "\n",
    "# Reshape the output to match the required format\n",
    "output_data = forecast_data[['Company_name', 'Year']].copy()\n",
    "output_data['Parameter'] = 'Repurchase of Common Stock'\n",
    "output_data['Value'] = forecast_data['Repurchase_Predicted']\n",
    "\n",
    "# Save the results\n",
    "output_path = 'repurchase_predictions_2024_2033.xlsx'\n",
    "output_data.to_excel(output_path, index=False)\n",
    "print(f\"Predictions saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data head:\n",
      "  Company_name                                   Parameters    2019    2020  \\\n",
      "0          BKR                   repurchase of common stock     0.0     0.0   \n",
      "1          BKR                               dividends paid  -630.0  -278.0   \n",
      "2          BKR  net cash flows used in financing activities  -695.0 -1007.0   \n",
      "3          BKR  net cash flows used in investing activities -1445.0  -486.0   \n",
      "4          BKR     proceeds from sales of equity securities  2669.0  2666.0   \n",
      "\n",
      "     2021    2022    2023  \n",
      "0  -434.0  -828.0  -538.0  \n",
      "1  -161.0  -435.0  -576.0  \n",
      "2  -838.0 -1798.0 -1671.0  \n",
      "3  -534.0  -967.0 -1659.0  \n",
      "4  2665.0  2664.0  2663.0  \n",
      "\n",
      "Forecast data head:\n",
      "  Company_name                                   Parameters      2024  \\\n",
      "0          BKR                               dividends paid  -570.824   \n",
      "1          BKR  net cash flows used in financing activities -1917.007   \n",
      "2          BKR  net cash flows used in investing activities -1714.013   \n",
      "3          BKR     proceeds from sales of equity securities  2661.496   \n",
      "4          BKR                         total current assets -1381.482   \n",
      "\n",
      "       2025      2026      2027       2028       2029       2030       2031  \\\n",
      "0  -534.631  -509.237  -497.090   -486.353   -490.984   -487.396   -463.032   \n",
      "1 -2133.827 -2367.924 -2611.155  -2870.269  -3100.475  -3349.806  -3584.737   \n",
      "2 -1792.028 -1824.691 -1899.551  -1928.034  -1989.011  -2040.778  -2145.299   \n",
      "3  2660.008  2658.518  2657.004   2655.510   2654.016   2652.564   2651.018   \n",
      "4 -3988.433 -6481.101 -8910.690 -11031.527 -13706.057 -16286.657 -19242.320   \n",
      "\n",
      "        2032       2033  \n",
      "0   -451.536   -438.121  \n",
      "1  -3823.404  -4060.690  \n",
      "2  -2261.899  -2280.911  \n",
      "3   2649.566   2648.072  \n",
      "4 -21227.288 -23950.316  \n",
      "\n",
      "Unique values in Parameters column of training data:\n",
      "['repurchase of common stock' 'dividends paid'\n",
      " 'net cash flows used in financing activities'\n",
      " 'net cash flows used in investing activities'\n",
      " 'proceeds from sales of equity securities' 'total current assets'\n",
      " 'all other current assets' 'other intangible assets, net'\n",
      " 'other non-operating income' 'proceeds from issuance of long-term debt'\n",
      " 'property, plant, and equipment less accumulated depreciation'\n",
      " 'accumulated other comprehensive loss' 'contract & other deferred assets'\n",
      " 'depreciation & amortization' 'net cash paid for acquisitions'\n",
      " 'changes in operating assets & liabilities (total)'\n",
      " 'cost of services sold' 'gross profit' 'sales of services'\n",
      " 'ebitda margin' 'increase (decrease) in cash & cash equivalents'\n",
      " 'operating cash flow margin' 'operating income' 'sales of goods'\n",
      " 'all other current liabilities']\n",
      "\n",
      "Unique values in Parameters column of forecast data:\n",
      "['dividends paid' 'net cash flows used in financing activities'\n",
      " 'net cash flows used in investing activities'\n",
      " 'proceeds from sales of equity securities' 'total current assets'\n",
      " 'all other current assets' 'other intangible assets, net'\n",
      " 'other non-operating income' 'proceeds from issuance of long-term debt'\n",
      " 'property, plant, and equipment less accumulated depreciation'\n",
      " 'accumulated other comprehensive loss' 'contract & other deferred assets'\n",
      " 'depreciation & amortization' 'net cash paid for acquisitions'\n",
      " 'changes in operating assets & liabilities (total)'\n",
      " 'cost of services sold' 'gross profit' 'sales of services'\n",
      " 'ebitda margin' 'increase (decrease) in cash & cash equivalents'\n",
      " 'operating cash flow margin' 'operating income' 'sales of goods'\n",
      " 'all other current liabilities']\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 429ms/step - loss: 1594497.1250 - mae: 532.7328 - val_loss: 5266381.5000 - val_mae: 1330.6180\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 1666685.7500 - mae: 585.1063 - val_loss: 5265975.5000 - val_mae: 1330.5443\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 1088871.0000 - mae: 477.0102 - val_loss: 5265594.5000 - val_mae: 1330.4740\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 1647408.6250 - mae: 564.2136 - val_loss: 5265201.0000 - val_mae: 1330.4012\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 1622556.1250 - mae: 565.4702 - val_loss: 5264820.5000 - val_mae: 1330.3303\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1671172.8750 - mae: 593.3452 - val_loss: 5264440.5000 - val_mae: 1330.2593\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 1095915.7500 - mae: 483.9811 - val_loss: 5264071.5000 - val_mae: 1330.1896\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 1670879.6250 - mae: 596.8334 - val_loss: 5263692.5000 - val_mae: 1330.1188\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 1100616.8750 - mae: 491.5035 - val_loss: 5263324.5000 - val_mae: 1330.0491\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - loss: 1088800.2500 - mae: 480.5336 - val_loss: 5262930.5000 - val_mae: 1329.9757\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 1662512.5000 - mae: 589.5356 - val_loss: 5262518.5000 - val_mae: 1329.8998\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1634945.7500 - mae: 552.3806 - val_loss: 5262107.5000 - val_mae: 1329.8234\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 1597848.3750 - mae: 533.7817 - val_loss: 5261694.5000 - val_mae: 1329.7462\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 1661102.0000 - mae: 585.4751 - val_loss: 5261278.5000 - val_mae: 1329.6666\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 1659574.6250 - mae: 580.3192 - val_loss: 5260869.0000 - val_mae: 1329.5873\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 1622334.0000 - mae: 562.3147 - val_loss: 5260459.5000 - val_mae: 1329.5073\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 1651308.7500 - mae: 572.0629 - val_loss: 5260038.5000 - val_mae: 1329.4232\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 1624727.8750 - mae: 571.1960 - val_loss: 5259615.5000 - val_mae: 1329.3381\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1034964.1875 - mae: 440.8793 - val_loss: 5259176.5000 - val_mae: 1329.2490\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 1609358.5000 - mae: 551.3257 - val_loss: 5258673.5000 - val_mae: 1329.1498\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - loss: 1658050.0000 - mae: 579.4871 - val_loss: 5258138.0000 - val_mae: 1329.0466\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 1609230.3750 - mae: 553.8865 - val_loss: 5257591.5000 - val_mae: 1328.9421\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 1652405.5000 - mae: 573.6528 - val_loss: 5257017.5000 - val_mae: 1328.8320\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 1072251.0000 - mae: 452.3287 - val_loss: 5256413.0000 - val_mae: 1328.7169\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 1069658.2500 - mae: 448.3463 - val_loss: 5255741.5000 - val_mae: 1328.5909\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 1040576.5625 - mae: 438.4239 - val_loss: 5255017.5000 - val_mae: 1328.4564\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 1660161.3750 - mae: 590.4198 - val_loss: 5254236.0000 - val_mae: 1328.3136\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 1651096.8750 - mae: 568.6453 - val_loss: 5253458.5000 - val_mae: 1328.1700\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 1567230.8750 - mae: 531.7249 - val_loss: 5252667.0000 - val_mae: 1328.0223\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - loss: 1603993.1250 - mae: 548.9717 - val_loss: 5251824.5000 - val_mae: 1327.8643\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 1607940.8750 - mae: 540.3364 - val_loss: 5250941.5000 - val_mae: 1327.6980\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - loss: 1645158.5000 - mae: 565.3806 - val_loss: 5250029.5000 - val_mae: 1327.5270\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 1067695.6250 - mae: 451.7392 - val_loss: 5249102.0000 - val_mae: 1327.3540\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 1650383.5000 - mae: 567.9865 - val_loss: 5248086.5000 - val_mae: 1327.1705\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - loss: 1095551.5000 - mae: 486.3263 - val_loss: 5247054.5000 - val_mae: 1326.9843\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 1626323.1250 - mae: 570.2888 - val_loss: 5245905.0000 - val_mae: 1326.7816\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 1620335.1250 - mae: 562.7544 - val_loss: 5244705.5000 - val_mae: 1326.5714\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 1584755.7500 - mae: 518.4349 - val_loss: 5243454.0000 - val_mae: 1326.3529\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 1067848.7500 - mae: 454.4576 - val_loss: 5242138.5000 - val_mae: 1326.1224\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 1662044.5000 - mae: 588.0864 - val_loss: 5240715.0000 - val_mae: 1325.8773\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - loss: 1575040.6250 - mae: 539.5502 - val_loss: 5239275.0000 - val_mae: 1325.6288\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1052317.5000 - mae: 464.2356 - val_loss: 5237758.0000 - val_mae: 1325.3671\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 1001676.0000 - mae: 428.6371 - val_loss: 5236131.5000 - val_mae: 1325.0907\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1603373.0000 - mae: 553.8584 - val_loss: 5234337.0000 - val_mae: 1324.7896\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - loss: 1079779.1250 - mae: 473.9146 - val_loss: 5232466.5000 - val_mae: 1324.4758\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1083223.6250 - mae: 480.6054 - val_loss: 5230452.5000 - val_mae: 1324.1417\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 1593099.0000 - mae: 537.0526 - val_loss: 5228304.0000 - val_mae: 1323.7913\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 1606331.1250 - mae: 552.3017 - val_loss: 5226100.5000 - val_mae: 1323.4296\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 1610007.8750 - mae: 559.7520 - val_loss: 5223845.5000 - val_mae: 1323.0571\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 1550545.0000 - mae: 512.9728 - val_loss: 5221534.5000 - val_mae: 1322.6765\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No matching data found in forecast data for 'Repurchase of Common Stock'. Check the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7028\\24144246.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[0mforecast_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforecast_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mforecast_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Parameters'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'repurchase of common stock'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mforecast_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No matching data found in forecast data for 'Repurchase of Common Stock'. Check the dataset.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m forecast_data = forecast_data.melt(id_vars=['Company_name', 'Parameters'], \n",
      "\u001b[1;31mValueError\u001b[0m: No matching data found in forecast data for 'Repurchase of Common Stock'. Check the dataset."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load datasets\n",
    "train_data_path = 'top 5+ stock 2019-2023.xlsx'\n",
    "forecast_data_path = 'top 5 2024_2033.xlsx'\n",
    "\n",
    "# Read the data\n",
    "train_data = pd.ExcelFile(train_data_path).parse(0)  # Assuming the first sheet contains data\n",
    "forecast_data = pd.ExcelFile(forecast_data_path).parse(0)  # Assuming the first sheet contains data\n",
    "\n",
    "# Inspect the data\n",
    "print(\"Training data head:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nForecast data head:\")\n",
    "print(forecast_data.head())\n",
    "\n",
    "# Check unique values in the Parameters column\n",
    "print(\"\\nUnique values in Parameters column of training data:\")\n",
    "print(train_data['Parameters'].unique())\n",
    "\n",
    "print(\"\\nUnique values in Parameters column of forecast data:\")\n",
    "print(forecast_data['Parameters'].unique())\n",
    "\n",
    "# Filter and reshape the training data\n",
    "train_data = train_data[train_data['Parameters'].str.strip().str.lower() == 'repurchase of common stock']\n",
    "if train_data.empty:\n",
    "    raise ValueError(\"No matching data found in training data for 'Repurchase of Common Stock'. Check the dataset.\")\n",
    "\n",
    "train_data = train_data.melt(id_vars=['Company_name', 'Parameters'], \n",
    "                             var_name='Year', \n",
    "                             value_name='Repurchase')\n",
    "train_data['Year'] = train_data['Year'].astype(int)  # Ensure year is an integer\n",
    "\n",
    "# Select features and target\n",
    "X = train_data[['Company_name', 'Year']]  # Features (Company name and Year for simplicity in this example)\n",
    "y = train_data['Repurchase']  # Target variable\n",
    "\n",
    "# Encode categorical variables (if any)\n",
    "X = pd.get_dummies(X, columns=['Company_name'], drop_first=True)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "# Preprocess the forecast data\n",
    "forecast_data = forecast_data[forecast_data['Parameters'].str.strip().str.lower() == 'repurchase of common stock']\n",
    "if forecast_data.empty:\n",
    "    raise ValueError(\"No matching data found in forecast data for 'Repurchase of Common Stock'. Check the dataset.\")\n",
    "\n",
    "forecast_data = forecast_data.melt(id_vars=['Company_name', 'Parameters'], \n",
    "                                   var_name='Year', \n",
    "                                   value_name='Repurchase')\n",
    "forecast_data['Year'] = forecast_data['Year'].astype(int)\n",
    "\n",
    "forecast_X = forecast_data[['Company_name', 'Year']]\n",
    "forecast_X = pd.get_dummies(forecast_X, columns=['Company_name'], drop_first=True)\n",
    "\n",
    "# Align columns in forecast data with training data\n",
    "forecast_X = forecast_X.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Scale the features\n",
    "forecast_X_scaled = scaler.transform(forecast_X)\n",
    "\n",
    "# Predict repurchase values for 2024-2033\n",
    "forecast_data['Repurchase_Predicted'] = model.predict(forecast_X_scaled)\n",
    "\n",
    "# Reshape the output to match the required format\n",
    "output_data = forecast_data[['Company_name', 'Year']].copy()\n",
    "output_data['Parameter'] = 'Repurchase of Common Stock'\n",
    "output_data['Value'] = forecast_data['Repurchase_Predicted']\n",
    "\n",
    "# Save the results\n",
    "output_path = 'repurchase_predictions_2024_2033.xlsx'\n",
    "output_data.to_excel(output_path, index=False)\n",
    "print(f\"Predictions saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data head:\n",
      "  Company_name                                   Parameters    2019    2020  \\\n",
      "0          BKR                   repurchase of common stock     0.0     0.0   \n",
      "1          BKR                               dividends paid  -630.0  -278.0   \n",
      "2          BKR  net cash flows used in financing activities  -695.0 -1007.0   \n",
      "3          BKR  net cash flows used in investing activities -1445.0  -486.0   \n",
      "4          BKR     proceeds from sales of equity securities  2669.0  2666.0   \n",
      "\n",
      "     2021    2022    2023  \n",
      "0  -434.0  -828.0  -538.0  \n",
      "1  -161.0  -435.0  -576.0  \n",
      "2  -838.0 -1798.0 -1671.0  \n",
      "3  -534.0  -967.0 -1659.0  \n",
      "4  2665.0  2664.0  2663.0  \n",
      "\n",
      "Forecast data head:\n",
      "  Company_name                                   Parameters      2024  \\\n",
      "0          BKR                               dividends paid  -570.824   \n",
      "1          BKR  net cash flows used in financing activities -1917.007   \n",
      "2          BKR  net cash flows used in investing activities -1714.013   \n",
      "3          BKR     proceeds from sales of equity securities  2661.496   \n",
      "4          BKR                         total current assets -1381.482   \n",
      "\n",
      "       2025      2026      2027       2028       2029       2030       2031  \\\n",
      "0  -534.631  -509.237  -497.090   -486.353   -490.984   -487.396   -463.032   \n",
      "1 -2133.827 -2367.924 -2611.155  -2870.269  -3100.475  -3349.806  -3584.737   \n",
      "2 -1792.028 -1824.691 -1899.551  -1928.034  -1989.011  -2040.778  -2145.299   \n",
      "3  2660.008  2658.518  2657.004   2655.510   2654.016   2652.564   2651.018   \n",
      "4 -3988.433 -6481.101 -8910.690 -11031.527 -13706.057 -16286.657 -19242.320   \n",
      "\n",
      "        2032       2033  \n",
      "0   -451.536   -438.121  \n",
      "1  -3823.404  -4060.690  \n",
      "2  -2261.899  -2280.911  \n",
      "3   2649.566   2648.072  \n",
      "4 -21227.288 -23950.316  \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386ms/step - loss: 1580342.6250 - mae: 534.7938 - val_loss: 5268265.0000 - val_mae: 1330.7869\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 1576741.0000 - mae: 529.0696 - val_loss: 5268058.0000 - val_mae: 1330.7369\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 1613204.7500 - mae: 548.3751 - val_loss: 5267843.0000 - val_mae: 1330.6848\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1677623.1250 - mae: 607.8572 - val_loss: 5267628.5000 - val_mae: 1330.6327\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 1613751.0000 - mae: 550.0004 - val_loss: 5267411.5000 - val_mae: 1330.5807\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1051765.0000 - mae: 459.0459 - val_loss: 5267194.0000 - val_mae: 1330.5289\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 1101843.5000 - mae: 493.1566 - val_loss: 5266962.5000 - val_mae: 1330.4750\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 1676318.8750 - mae: 604.3073 - val_loss: 5266733.5000 - val_mae: 1330.4213\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 1613648.3750 - mae: 561.2164 - val_loss: 5266509.5000 - val_mae: 1330.3678\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 1647701.1250 - mae: 566.8112 - val_loss: 5266297.0000 - val_mae: 1330.3143\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 1595445.6250 - mae: 525.0556 - val_loss: 5266065.5000 - val_mae: 1330.2578\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 1666159.3750 - mae: 590.1230 - val_loss: 5265820.5000 - val_mae: 1330.1984\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1007385.3125 - mae: 427.8372 - val_loss: 5265582.0000 - val_mae: 1330.1395\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - loss: 971209.7500 - mae: 386.7418 - val_loss: 5265313.0000 - val_mae: 1330.0762\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 1054173.2500 - mae: 461.3812 - val_loss: 5265008.5000 - val_mae: 1330.0082\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 1665203.3750 - mae: 587.9539 - val_loss: 5264685.5000 - val_mae: 1329.9384\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1605665.6250 - mae: 546.4362 - val_loss: 5264363.5000 - val_mae: 1329.8680\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1627363.8750 - mae: 568.8669 - val_loss: 5264023.5000 - val_mae: 1329.7932\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - loss: 1619063.3750 - mae: 554.3621 - val_loss: 5263665.5000 - val_mae: 1329.7153\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1644860.2500 - mae: 561.5411 - val_loss: 5263289.5000 - val_mae: 1329.6353\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - loss: 1029617.3125 - mae: 437.5594 - val_loss: 5262905.0000 - val_mae: 1329.5529\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - loss: 1565989.8750 - mae: 523.6003 - val_loss: 5262455.5000 - val_mae: 1329.4603\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 1656392.5000 - mae: 571.0999 - val_loss: 5261969.5000 - val_mae: 1329.3605\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 1660388.2500 - mae: 579.3468 - val_loss: 5261479.0000 - val_mae: 1329.2603\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - loss: 1606523.8750 - mae: 542.5449 - val_loss: 5260976.0000 - val_mae: 1329.1573\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 1619120.7500 - mae: 556.5518 - val_loss: 5260434.0000 - val_mae: 1329.0460\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - loss: 1652041.3750 - mae: 579.6414 - val_loss: 5259855.0000 - val_mae: 1328.9280\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 1607270.3750 - mae: 545.0723 - val_loss: 5259255.5000 - val_mae: 1328.8064\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 1032663.5625 - mae: 442.0771 - val_loss: 5258612.5000 - val_mae: 1328.6761\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 1617128.1250 - mae: 563.2589 - val_loss: 5257879.5000 - val_mae: 1328.5317\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1619727.5000 - mae: 563.6168 - val_loss: 5257099.5000 - val_mae: 1328.3795\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - loss: 1033958.0625 - mae: 441.6882 - val_loss: 5256284.0000 - val_mae: 1328.2192\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 1661551.5000 - mae: 579.2828 - val_loss: 5255374.0000 - val_mae: 1328.0436\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 1072586.2500 - mae: 463.9466 - val_loss: 5254446.5000 - val_mae: 1327.8656\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 1093686.7500 - mae: 485.9799 - val_loss: 5253426.5000 - val_mae: 1327.6732\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1012570.8125 - mae: 419.1678 - val_loss: 5252334.5000 - val_mae: 1327.4716\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 1067600.1250 - mae: 452.2578 - val_loss: 5251136.5000 - val_mae: 1327.2521\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 1013981.9375 - mae: 413.1336 - val_loss: 5249879.5000 - val_mae: 1327.0250\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 1560672.8750 - mae: 521.8083 - val_loss: 5248518.0000 - val_mae: 1326.7812\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 1052177.3750 - mae: 458.1368 - val_loss: 5247104.0000 - val_mae: 1326.5284\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1668495.3750 - mae: 600.7230 - val_loss: 5245601.5000 - val_mae: 1326.2632\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 1619756.1250 - mae: 558.0909 - val_loss: 5244101.5000 - val_mae: 1325.9979\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1610156.6250 - mae: 552.2860 - val_loss: 5242556.5000 - val_mae: 1325.7246\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 1597385.2500 - mae: 541.3848 - val_loss: 5240954.0000 - val_mae: 1325.4401\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - loss: 1603871.8750 - mae: 551.6214 - val_loss: 5239281.5000 - val_mae: 1325.1440\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - loss: 1603506.8750 - mae: 538.2480 - val_loss: 5237540.0000 - val_mae: 1324.8344\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 1550397.1250 - mae: 515.0856 - val_loss: 5235720.0000 - val_mae: 1324.5145\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 1032843.8125 - mae: 436.9084 - val_loss: 5233794.5000 - val_mae: 1324.1790\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - loss: 1611875.6250 - mae: 563.5955 - val_loss: 5231667.0000 - val_mae: 1323.8157\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1030659.8750 - mae: 435.5991 - val_loss: 5229493.0000 - val_mae: 1323.4449\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No matching data found in forecast data for 'Repurchase of Common Stock'. Available parameters: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7028\\2025446291.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mforecast_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     raise ValueError(f\"No matching data found in forecast data for 'Repurchase of Common Stock'. \"\n\u001b[0m\u001b[0;32m     69\u001b[0m                      f\"Available parameters: {forecast_data['Parameters'].unique()}\")\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No matching data found in forecast data for 'Repurchase of Common Stock'. Available parameters: []"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load datasets\n",
    "train_data_path = 'top 5+ stock 2019-2023.xlsx'\n",
    "forecast_data_path = 'top 5 2024_2033.xlsx'\n",
    "\n",
    "# Read the data\n",
    "train_data = pd.ExcelFile(train_data_path).parse(0)  # Assuming the first sheet contains data\n",
    "forecast_data = pd.ExcelFile(forecast_data_path).parse(0)  # Assuming the first sheet contains data\n",
    "\n",
    "# Inspect the data\n",
    "print(\"Training data head:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nForecast data head:\")\n",
    "print(forecast_data.head())\n",
    "\n",
    "# Normalize and filter training data\n",
    "train_data['Parameters'] = train_data['Parameters'].str.strip().str.lower()\n",
    "train_data = train_data[train_data['Parameters'] == 'repurchase of common stock']\n",
    "\n",
    "if train_data.empty:\n",
    "    raise ValueError(\"No matching data found in training data for 'Repurchase of Common Stock'. Check the dataset formatting.\")\n",
    "\n",
    "# Reshape training data\n",
    "train_data = train_data.melt(id_vars=['Company_name', 'Parameters'], \n",
    "                             var_name='Year', \n",
    "                             value_name='Repurchase')\n",
    "train_data['Year'] = train_data['Year'].astype(int)\n",
    "\n",
    "# Select features and target\n",
    "X = train_data[['Company_name', 'Year']]\n",
    "y = train_data['Repurchase']\n",
    "\n",
    "# Encode categorical variables\n",
    "X = pd.get_dummies(X, columns=['Company_name'], drop_first=True)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "# Normalize and filter forecast data\n",
    "forecast_data['Parameters'] = forecast_data['Parameters'].str.strip().str.lower()\n",
    "forecast_data = forecast_data[forecast_data['Parameters'] == 'repurchase of common stock']\n",
    "\n",
    "if forecast_data.empty:\n",
    "    raise ValueError(f\"No matching data found in forecast data for 'Repurchase of Common Stock'. \"\n",
    "                     f\"Available parameters: {forecast_data['Parameters'].unique()}\")\n",
    "\n",
    "# Reshape forecast data\n",
    "forecast_data = forecast_data.melt(id_vars=['Company_name', 'Parameters'], \n",
    "                                   var_name='Year', \n",
    "                                   value_name='Repurchase')\n",
    "forecast_data['Year'] = forecast_data['Year'].astype(int)\n",
    "\n",
    "# Prepare features for prediction\n",
    "forecast_X = forecast_data[['Company_name', 'Year']]\n",
    "forecast_X = pd.get_dummies(forecast_X, columns=['Company_name'], drop_first=True)\n",
    "\n",
    "# Align columns with training data\n",
    "missing_cols = set(X.columns) - set(forecast_X.columns)\n",
    "for col in missing_cols:\n",
    "    forecast_X[col] = 0  # Add missing columns as zeros\n",
    "\n",
    "forecast_X = forecast_X[X.columns]  # Ensure same column order\n",
    "forecast_X_scaled = scaler.transform(forecast_X)\n",
    "\n",
    "# Predict repurchase values for 2024-2033\n",
    "forecast_data['Repurchase_Predicted'] = model.predict(forecast_X_scaled)\n",
    "\n",
    "# Reshape the output to match the required format\n",
    "output_data = forecast_data[['Company_name', 'Year']].copy()\n",
    "output_data['Parameter'] = 'Repurchase of Common Stock'\n",
    "output_data['Value'] = forecast_data['Repurchase_Predicted']\n",
    "\n",
    "# Save the results\n",
    "output_path = 'repurchase_predictions_2024_2033.xlsx'\n",
    "output_data.to_excel(output_path, index=False)\n",
    "print(f\"Predictions saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 1.2923 - mae: 1.1199 - val_loss: 1.2412 - val_mae: 1.1133\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 1.1540 - mae: 1.0525 - val_loss: 1.1356 - val_mae: 1.0649\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 1.0951 - mae: 1.0319 - val_loss: 1.0398 - val_mae: 1.0191\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - loss: 0.9537 - mae: 0.9602 - val_loss: 0.9496 - val_mae: 0.9740\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 0.8295 - mae: 0.8899 - val_loss: 0.8652 - val_mae: 0.9297\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 0.7931 - mae: 0.8724 - val_loss: 0.7864 - val_mae: 0.8864\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step - loss: 0.7432 - mae: 0.8505 - val_loss: 0.7139 - val_mae: 0.8446\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 0.6859 - mae: 0.8116 - val_loss: 0.6450 - val_mae: 0.8028\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - loss: 0.5754 - mae: 0.7441 - val_loss: 0.5796 - val_mae: 0.7611\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - loss: 0.5761 - mae: 0.7427 - val_loss: 0.5212 - val_mae: 0.7218\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 0.4779 - mae: 0.6729 - val_loss: 0.4654 - val_mae: 0.6821\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - loss: 0.4412 - mae: 0.6527 - val_loss: 0.4134 - val_mae: 0.6429\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 0.3835 - mae: 0.6125 - val_loss: 0.3639 - val_mae: 0.6031\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - loss: 0.3393 - mae: 0.5767 - val_loss: 0.3170 - val_mae: 0.5629\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - loss: 0.2877 - mae: 0.5312 - val_loss: 0.2724 - val_mae: 0.5218\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - loss: 0.2886 - mae: 0.5279 - val_loss: 0.2304 - val_mae: 0.4799\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - loss: 0.2284 - mae: 0.4699 - val_loss: 0.1915 - val_mae: 0.4374\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 0.1902 - mae: 0.4252 - val_loss: 0.1556 - val_mae: 0.3944\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - loss: 0.1514 - mae: 0.3662 - val_loss: 0.1234 - val_mae: 0.3511\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - loss: 0.1777 - mae: 0.3775 - val_loss: 0.0951 - val_mae: 0.3082\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - loss: 0.1483 - mae: 0.3679 - val_loss: 0.0712 - val_mae: 0.2665\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - loss: 0.0887 - mae: 0.2536 - val_loss: 0.0515 - val_mae: 0.2264\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - loss: 0.0965 - mae: 0.2625 - val_loss: 0.0354 - val_mae: 0.1872\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - loss: 0.0974 - mae: 0.2239 - val_loss: 0.0228 - val_mae: 0.1497\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - loss: 0.1001 - mae: 0.2063 - val_loss: 0.0136 - val_mae: 0.1144\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step - loss: 0.0843 - mae: 0.1886 - val_loss: 0.0074 - val_mae: 0.0821\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - loss: 0.0623 - mae: 0.1540 - val_loss: 0.0034 - val_mae: 0.0519\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 0.0870 - mae: 0.1942 - val_loss: 0.0014 - val_mae: 0.0314\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 0.0584 - mae: 0.1616 - val_loss: 8.8147e-04 - val_mae: 0.0219\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378ms/step - loss: 0.1116 - mae: 0.1794 - val_loss: 0.0013 - val_mae: 0.0300\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - loss: 0.0986 - mae: 0.1804 - val_loss: 0.0021 - val_mae: 0.0375\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 0.0925 - mae: 0.1754 - val_loss: 0.0027 - val_mae: 0.0441\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496ms/step - loss: 0.1112 - mae: 0.1951 - val_loss: 0.0031 - val_mae: 0.0488\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - loss: 0.1969 - mae: 0.2526 - val_loss: 0.0028 - val_mae: 0.0466\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 0.1083 - mae: 0.2010 - val_loss: 0.0023 - val_mae: 0.0416\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - loss: 0.1304 - mae: 0.2263 - val_loss: 0.0015 - val_mae: 0.0336\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 0.0601 - mae: 0.1769 - val_loss: 8.1605e-04 - val_mae: 0.0265\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 0.1046 - mae: 0.1755 - val_loss: 4.3457e-04 - val_mae: 0.0186\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - loss: 0.0550 - mae: 0.1450 - val_loss: 4.5634e-04 - val_mae: 0.0161\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 0.0980 - mae: 0.1992 - val_loss: 9.4194e-04 - val_mae: 0.0243\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - loss: 0.0552 - mae: 0.1715 - val_loss: 0.0019 - val_mae: 0.0384\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - loss: 0.0767 - mae: 0.1624 - val_loss: 0.0033 - val_mae: 0.0528\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - loss: 0.0959 - mae: 0.1518 - val_loss: 0.0052 - val_mae: 0.0679\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 0.1005 - mae: 0.1715 - val_loss: 0.0075 - val_mae: 0.0820\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 0.0628 - mae: 0.1580 - val_loss: 0.0098 - val_mae: 0.0946\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - loss: 0.0547 - mae: 0.1664 - val_loss: 0.0123 - val_mae: 0.1062\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 0.0751 - mae: 0.1945 - val_loss: 0.0147 - val_mae: 0.1162\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - loss: 0.0559 - mae: 0.1447 - val_loss: 0.0171 - val_mae: 0.1251\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - loss: 0.0687 - mae: 0.1792 - val_loss: 0.0193 - val_mae: 0.1329\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 0.0606 - mae: 0.1675 - val_loss: 0.0211 - val_mae: 0.1387\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No matching data found in forecast data for 'repurchase of common stock'.\nAvailable parameters: ['dividends paid' 'net cash flows used in financing activities'\n 'net cash flows used in investing activities'\n 'proceeds from sales of equity securities' 'total current assets'\n 'all other current assets' 'other intangible assets, net'\n 'other non-operating income' 'proceeds from issuance of long-term debt'\n 'property, plant, and equipment less accumulated depreciation'\n 'accumulated other comprehensive loss' 'contract & other deferred assets'\n 'depreciation & amortization' 'net cash paid for acquisitions'\n 'changes in operating assets & liabilities (total)'\n 'cost of services sold' 'gross profit' 'sales of services'\n 'ebitda margin' 'increase (decrease) in cash & cash equivalents'\n 'operating cash flow margin' 'operating income' 'sales of goods'\n 'all other current liabilities']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7028\\1911327052.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mparameter_to_forecast\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mforecast_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Parameters\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mavailable_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforecast_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Parameters\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[1;34mf\"No matching data found in forecast data for '{parameter_to_forecast}'.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;34mf\"Available parameters: {available_parameters}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No matching data found in forecast data for 'repurchase of common stock'.\nAvailable parameters: ['dividends paid' 'net cash flows used in financing activities'\n 'net cash flows used in investing activities'\n 'proceeds from sales of equity securities' 'total current assets'\n 'all other current assets' 'other intangible assets, net'\n 'other non-operating income' 'proceeds from issuance of long-term debt'\n 'property, plant, and equipment less accumulated depreciation'\n 'accumulated other comprehensive loss' 'contract & other deferred assets'\n 'depreciation & amortization' 'net cash paid for acquisitions'\n 'changes in operating assets & liabilities (total)'\n 'cost of services sold' 'gross profit' 'sales of services'\n 'ebitda margin' 'increase (decrease) in cash & cash equivalents'\n 'operating cash flow margin' 'operating income' 'sales of goods'\n 'all other current liabilities']"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Load the training data\n",
    "training_data = pd.read_excel(\"top 5+ stock 2019-2023.xlsx\")\n",
    "\n",
    "# Normalize parameter names in training data\n",
    "training_data[\"Parameters\"] = training_data[\"Parameters\"].str.lower().str.strip()\n",
    "\n",
    "# Transform the data for training\n",
    "training_data_melted = training_data.melt(\n",
    "    id_vars=[\"Company_name\", \"Parameters\"], \n",
    "    var_name=\"Year\", \n",
    "    value_name=\"Value\"\n",
    ")\n",
    "\n",
    "# Pivot the data to have years as columns\n",
    "training_data_pivot = training_data_melted.pivot_table(\n",
    "    index=[\"Company_name\", \"Parameters\"],\n",
    "    columns=\"Year\",\n",
    "    values=\"Value\"\n",
    ").reset_index()\n",
    "\n",
    "# Extract features and target columns\n",
    "features = training_data_pivot.iloc[:, 2:-1].values  # All years except the last one\n",
    "target = training_data_pivot.iloc[:, -1].values  # The last year as the target\n",
    "\n",
    "# Normalize the features and target\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "target_scaled = scaler.fit_transform(target.reshape(-1, 1))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_scaled, target_scaled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, activation=\"relu\", input_dim=X_train.shape[1]),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation=\"linear\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Load the forecast data\n",
    "forecast_data = pd.read_excel(\"top 5 2024_2033.xlsx\")\n",
    "\n",
    "# Normalize parameter names in forecast data\n",
    "forecast_data[\"Parameters\"] = forecast_data[\"Parameters\"].str.lower().str.strip()\n",
    "\n",
    "# Define the parameter to forecast\n",
    "parameter_to_forecast = \"repurchase of common stock\".lower().strip()\n",
    "\n",
    "# Check if the parameter exists in the forecast data\n",
    "if parameter_to_forecast not in forecast_data[\"Parameters\"].values:\n",
    "    available_parameters = forecast_data[\"Parameters\"].unique()\n",
    "    raise ValueError(\n",
    "        f\"No matching data found in forecast data for '{parameter_to_forecast}'.\\n\"\n",
    "        f\"Available parameters: {available_parameters}\"\n",
    "    )\n",
    "\n",
    "# Filter the forecast data for the desired parameter\n",
    "forecast_data_filtered = forecast_data[forecast_data[\"Parameters\"] == parameter_to_forecast]\n",
    "\n",
    "# Prepare the forecast features\n",
    "forecast_features = forecast_data_filtered.iloc[:, 2:].values\n",
    "forecast_features_scaled = scaler.transform(forecast_features)\n",
    "\n",
    "# Predict future values\n",
    "predicted_values_scaled = model.predict(forecast_features_scaled)\n",
    "\n",
    "# Inverse scale the predictions\n",
    "predicted_values = scaler.inverse_transform(predicted_values_scaled)\n",
    "\n",
    "# Add the predictions to the forecast data\n",
    "forecast_data_filtered.loc[:, \"Predicted Values\"] = predicted_values\n",
    "\n",
    "# Save the updated forecast data\n",
    "forecast_data_filtered.to_csv(\"forecasted_data_with_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Forecasting complete. Results saved to 'forecasted_data_with_predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
